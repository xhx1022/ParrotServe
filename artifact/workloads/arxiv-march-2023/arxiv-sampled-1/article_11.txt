
	
	
	1]Juan Chen
	1]Yingchun Zhou Corresponding author:  yczhou@stat.ecnu.edu.cn
	[1]Key Laboratory of Advanced Theory and Application in Statistics
		and Data Science-MOE, School of Statistics, East China Normal University.
	
	
	Weighted Euclidean balancing for a matrix exposure in estimating causal effect
    [
    
==============================================================================


		In many scientific fields such as biology, psychology and sociology, there is an increasing interest in estimating the causal effect of a matrix exposure on an outcome.  Covariate balancing is crucial in causal inference and both exact balancing and approximate balancing methods have been proposed in the past decades. However, due to the large number of constraints, it is difficult to achieve exact balance or to select the threshold parameters for approximate balancing methods when the treatment is a matrix. To meet these challenges, we propose the weighted Euclidean balancing method, which approximately balance covariates from an overall perspective. This method is also applicable to high-dimensional covariates scenario. Both parametric and nonparametric methods are proposed to estimate the causal effect of matrix treatment and theoretical properties of the two estimations are provided. Furthermore, the simulation results show that the proposed method outperforms other methods in various cases. Finally, the method is applied to investigating the causal relationship between children's participation in various training courses and their IQ. The results show that the duration of attending hands-on practice courses for children at 6-9 years old has a siginificantly positive impact on children's IQ.
	
	Keywords: causal inference, matrix treatment, weighting methods, overall imbalance, observational study. 
	
	

¬ß INTRODUCTION

	For decades, causal inference has been widely used in many fields, such as biology, psychology and economics, etc. Most of the current research is based on univariate treatment (binary, multivalued or continuous treatment)  (<cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>; <cit.>). However, one may be interested in the causal effect of a matrix treatment. For example, in studying the impact of children's participation in training courses on children's intelligence (measured by IQ), the exposure is a matrix, whose rows represent different age groups, columns represent the types of trainging courses and each element represents the number of hours of class per week. The existing methods are not suitable for matrix exposure and there have been few research on this. Therefore, the goal of this paper is to develop a new method to estimate the causal effect function for matrix exposure.
	To estimate causal effects in observational studies, it is common to use propensity scores (<cit.>; <cit.>; <cit.>). There are several classes of propensity score-based methods, such as matching, weighting and subclassification, that have become part of applied researchers' standard toolkit across many scientific displines (<cit.>; <cit.>). In this article we focus on the weighting method. 
	In the past decade, various weighting methods have been proposed to balance covariates in the estimation procedure (<cit.>; <cit.>; <cit.>). The key idea of these methods is to estimate propensity score ( <cit.>; <cit.>; <cit.>; <cit.>).
	
	When using the parametric method to model the propensity score, the estimation bias of the causal effect will be large if the model is mis-specified. Therefore, some nonparametric methods for estimating the propensity score have been proposed, such as the kernel density estimation (<cit.>). In addition, in recent years, some studies have used optimized weighting methods to directly optimize the balance of covariates (<cit.>; <cit.>; <cit.>).
	
	These methods avoid the direct construction of the propensity scores, therefore the obtained estimate achieves higher robustness. One of the methods, the entropy balancing method, has been established as being doubly robust, in that a consistent estimate can still be obtained when one of the two models, either the treatment assignment model or the outcome model, is correctly specified (<cit.>).
	
	Furthermore, this method can be easily implemented by solving a convex optimization problem. Here we extend the entropy balancing method to the matrix treatment scenario to balance the covariates.
	The methods mentioned above assume that all balancing conditions hold exactly, that is, they are exact balancing methods. However, the balancing conditions cannot hold exactly when the dimension of covariate or treatment is high as there will be too many equations to hold simultaneously. For matrix treatment, it is even more difficult for the balancing conditions to hold exactly. To meet this challenge, literatures have shown that approximate balance can trade bias for variance whereas exact balance cannot and the former works well in practice in both low- and high-dimensional settings (<cit.>; <cit.>). The potential limitations of the existing approximate balancing methods are that they directly control univariate imbalance, which cannot guarantee the overall balance especially in the high-dimensional scenario. Besides, there is no principled way to select the threshold parameters simultaneously in practice. Another potential issue of the univariate approximate balancing methods is that it is difficult to handle high-dimensional constraints since the theoretical results require that the number of the balancing constraints should be much smaller than the sample size(<cit.>).
	To alleviate the limitations of univariate balancing methods, we propose an overall balancing approach, which is called Weighted Euclidean balancing method. The weight is obtained by optimizing the entropy function subject to a single inequality constraint, hence the issue of tuning multiple threshold parameters in univariate balancing methods is solved. The Weighted Euclidean distance is defined to measure the overall imbalance and a sufficient small value of the distance suggests that the covariates are approximately balanced from the overall perspective. Moreover, we propose an algorithm to deal with high-dimensional constraints, so that the proposed method is not restrictive to the low-dimensional setting.
	The main contributions of the paper are summarized as follows. First, an overall balancing method (Weighted Euclidean balancing method) is proposed, which extends the binary univariate entropy approximate balancing method to the matrix treatment scenario. Unlike univariate approximate balancing method, the Weighted Euclidean balancing method controls the imbalance  from the overall perspective. Moreover, to the best of our knowledge, it is the first time that matrix treatment is studied by weighting method in causal inference literature. Second, both parametric and nonparametric causal effect estimation methods for matrix treatment are proposed. Under the parametric framework, a weighted optimization estimation is defined and its theoretical properties are provided. Under the nonparametric framework, B-splines are used to approximate the causal effect function and the convergence rate of the estimation is provided. Third, the proposed method is applied to explore the impact of children's participation in training courses on their IQ and meaningful results are obtained.
	The remainder of this article is organized as follows: In Section 2, the preliminaries are introduced. In Section 3, the Weighted Euclidean balancing method (WEBM) is proposed. In Section 4, the theoretical properties of the WEBM method are shown. In section 5, a numerical simulation is performed to evaluate the performance of the WEBM method under finite samples. In Section 6, the WEBM method is applied to analyze a real problem. The conclusions and discussions are summarized in Section 7.
	
	
	
	
	
	

¬ß PRELIMINARY

	

 ¬ß.¬ß Notation and assumptions

	Suppose an independent and identically distributed sample (ùêô_1,‚Ä¶,ùêô_n) is observed, where the support of ùêô = (ùêì,ùêó,Y) is ùíµ=(ùíØ√óùí≥√óùí¥). Here ùêì‚àà R^p√ó q denotes a matrix exposure, ùêó‚àà R^J denotes a vector of covariates, and Y ‚àà R denotes the observed outcome. Since the causal effect is characterized by potential outcome notion, let Y(t) for all t‚ààùíØ denotes the potential outcome that would be observed under treatment level ùê≠, i.e. Y = Y(t) if T= t.  
	
	In this paper, our goal is to estimate the causal effect function ùîº(Y(ùê≠)), which is defined in terms of potential outcomes that are not directly observed. Therefore, three assumptions that commonly employed for indentification are made (<cit.>; <cit.>).

	
	
	Assumption 1 (Ignorability):

	

	T_i ‚ä• Y_i(t) |X_i, which implies that the set of observed pre-treatment covariates ùêó_i, is sufficiently rich such that it includes all confounders , i.e. there is no unmeasured confounding.
	

	
	Assumption 2 (Positivity):

	

	f_T|X(T_i = t|X_i ) > 0 for all t‚ààùíØ, which means that treatment is not assigned deterministically.

	
	
	Assumption 3 (SUTVA):

	

	Assume that there is no interference among the units, which means that each individual's outcome depends only on their own level of treatment intensity.
	

	
	Under the above assumptions, we first define the stabilized weight as

	
    w_i = f(T_i)/f(T_i |X_i),

	then one can estimate the causal effect function based on the stabilized weight with observational data.
	
	

 ¬ß.¬ß Exact entropy balancing and approximate entropy balancing for matrix exposure

	The entropy balancing method (<cit.>)
	
	is used to determine the optimal weight for inferring causal effects. It has been used for univariate treatment and here this method is extended to matrix exposure and to balance covariates approximately.	
	Note that the stabilized weight 

	
    w_i = f(T_i)/f(T_i |X_i)

	satisfies the following conditions for any suitable functions u(ùêì) and v(ùêó):
	
    ùîº(w_iu(ùêì_i)v(ùêó_i)) =    ‚à¨f(T_i)/f(T_i |X_i)u(ùêì_i)v(ùêó_i)f(ùêì_i, X_i)dT_idX_i 
       =‚à´{f(T_i)/f(T_i |X_i)u(T_i)f(T_i |X_i)dT_i } v(X_i)f(X_i)dX_i 
       =ùîº(u(ùêì_i))ùîº(v(ùêó_i))

	Besides, it also satisfies that 
	
    ùîº(w_i) = ‚à¨f(T_i)/f(T_i |X_i)f(T_i, X_i)dT_idX_i = 1.

	However, Equation (2) implies an infinite number of moment conditions, which is impossible to solve with a finite sample of observations. Hence, the finite dimensional sieve space is considered to approximate the infinite dimensional function space. Specifically, let 
	
    u_K1(ùêì) = (u_K1,1(ùêì), u_K1,2(ùêì),‚Ä¶, u_K1,K1(ùêì))^', 
    
    		v_K2(ùêó) = (v_K2,1(ùêó), v_K2,2(ùêó), ‚Ä¶, v_K2,K2(ùêó))^'

	denote the known basis functions, then 
	
    ùîº(w_i u_K1(ùêì_i) v_K2(ùêó_i)^') = ùîº(u_K1(ùêì_i))ùîº(v_K2(ùêó_i)^').

	
	In practice, the covariate balancing conditions given in Equation (4) cannot hold exactly with high dimensional covariates or treatments. It is even more difficult to hold exactly for matrix exposure. To overcome this difficulty, approximate balance is considered rather than exact balance, which has been demonstrated to work well in practice in both low- and high-dimensional settings (<cit.>; <cit.>; <cit.>). Specifically, the balancing weights that approximately satisfy the conditions in Equation (4) are the global minimum of the following optimization problem:
	
    min_ùê∞‚àë_i=1^nw_ilog(w_i)

	s.t.
	
    |1/n‚àë_i=1^nw_i u_K1,l(ùêì_i)v_K2,lÃÉ(ùêó_i) - (1/n‚àë_i=1^n u_K1,l(ùêì_i)) (1/n‚àë_i=1^nv_K2,lÃÉ(ùêó_i)) |‚â§Œ¥_l,lÃÉ,

	where u_K1,l(ùêì_i) and v_K2,lÃÉ(ùêó_i) denote the lth and lÃÉth components of u_K1(ùêì_i) and v_K2(ùêó_i), respectively.
	Let m_K(ùêì_i, ùêó_i) = vec(1/n u_K1(ùêì_i)v_K2(ùêó_i)^') and mÃÖ_K = vec (1/nuÃÖ_K1vÃÖ_K2^')  denote two column vectors with dimension K, where K= K1 K2, the lth and lÃÉth components of uÃÖ_K1 and vÃÖ_K2 are defined as
	
    uÃÖ_K1,l = 1/n‚àë_i=1^n u_K1,l(ùêì_i)  and vÃÖ_K2,lÃÉ = 1/n‚àë_i=1^n v_K2,lÃÉ(ùêó_i),

	then condition (5) is equivalent to 
	
    min_w ‚àë_i=1^nw_ilog(w_i)

	s.t.
	
    |‚àë_i=1^nw_i m_K,k(ùêì_i, ùêó_i)  -  nmÃÖ_K,k|‚â§Œ¥_k,  k= 1,‚Ä¶,K.

	However, there is a large number of tuning parameters (Œ¥_1,‚Ä¶,Œ¥_K) which is very time-consuming to determine and there is lack of guideline on tuning these parameters simultaneously in practice. 
	
	
	

¬ß METHODOLOGY

	Due to the potential issues of univariate approximate balancing methods, the weighted Euclidean balancing method is proposed in this section, whose key idea is to control the overall imbalance in the optimization problem (7). 
	

 ¬ß.¬ß Weighted Euclidean balancing method

	
	Define the following weighted Euclidean imbalance measure (WEIM) as a weighted version of the squared Euclidean distance:
	
    WEIM = ‚àë_k=1^K{Œª_k^2 [‚àë_i=1^n w_i (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k)]^2 }.

	The weighted Euclidean balancing obtains the balancing weights that approximately satisfy the condition (4) by solving the following convex optimization problem:
	
    min_w ‚àë_i=1^nw_ilog(w_i)

	s.t.
	
    ‚àë_k=1^K{Œª_k^2 [‚àë_i=1^n w_i (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k)]^2 }‚â§Œ¥,

	where (Œª_1,‚Ä¶, Œª_K) is a pre-sepecified weight vector and Œ¥‚â• 0 is a threshold parameter. Assume that condition (3) holds exactly, whose sample condition is 1/n‚àë_i=1^n w_i = 1.
	
	
	Note that univariate exact balance is equivalent to the overall exact balance, in the sense that ‚àë_i=1^n w_i (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k)=0, k= 1,‚Ä¶,K  is equivalent to WEIM=0. However, the univariate approximate balance does not imply the overall approximate balance since it is possible that ‚àë_i=1^n w_i (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k), k= 1,‚Ä¶,K is small while the WEIM is large.
	The pre-specified vector  (Œª_1,‚Ä¶, Œª_K) reflects the importance of each univariate constraint. In this paper, we set Œª_k= œÉ_k^-1, where œÉ_k^2 is the variance of m_K,k(ùêì,ùêó). Since problem (9) is difficult to solve numerically, its dual problem is considered here, which can be solved by numerically efficient algorithms. Theorem 1 provides the dual formulation of problem (9) as an unconstrained problem.
	

	
	Theorem 1.  Assume that max_i (max_k |Œª_km_K,k(ùêì_i,ùêó_i) |) < ‚àû, the dual of problem (9) is equivalent to the following unconstrained problem 
	
    min_Œ∏‚àà R^K‚àë_i=1^nexp( ‚àë_k=1^KŒ∏_j Œª_j (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k))  + ‚àö(Œ¥)||Œ∏||_2,

	
	and the primal solution ≈µ_i is given by 
	
    ≈µ_i = exp{‚àë_k=1^KŒ∏ÃÇ_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k) -1} ,  i=1,‚Ä¶,n,

	where Œ∏ÃÇ is the solution to the dual optimization problem (10). 
	The proof of Theorem 1 is in Appendix A.1. 
	
	

  ¬ß.¬ß.¬ß Selection of tuning parameter

	Another practical issue that arises with weighted Euclidean weights is how to choose the degree of approximate balance Œ¥. A tuning algorithm is proposed as follows. First, determine a range of positive values ùíü for Œ¥, then the optimal value of Œ¥ is selected by the following algorithm.
	

	
	 Algorithm 1. Selection of Œ¥.

	For each Œ¥‚ààùíü,
	
		
  1.  Compute the dual parameters Œ∏ÃÇ by solving the dual problem (10);
		
  2.  Compute the estimated weights ≈µ_i using equation (11);
		
  3.  Calculate WEIM in (8) using ≈µ_i;
	
	Output Œ¥^* that minimizes WEIM.
	
	

  ¬ß.¬ß.¬ß Weighted Euclidean balancing with high-dimensional covariates

	In the high-dimensional or ultra high-dimensional covariate setting with K relatively large compared to n or K>>n, it becomes difficult to control the overall imbalance using the Weighted Euclidean balancing method. To meet this challenge, we propose an algorithm to select a small subset of the covariates in the sparse setting. Specifically, consider v_K2(ùêó) = (1, ùêó) in the high-dimensional setting. Let Bcor_j be the ball correlation (<cit.>) between X_j and ùêì, j=1,‚Ä¶,L. Rank X_1, ‚Ä¶,X_L as X_(1),‚Ä¶,X_(L) such that X_(1) has the largest Bcor value, X_(2) has the second largest Bcor value, and so forth. The covariates are added successively according to the rank of ball correlation until there is a break point of WEIM's, and the set added before the break point appears is the target set. The key idea hings on WEIM, which represents the contribution of the j most imbalanced covariates to the overall imbalance after WEBM weighting.  If WEIM remains stable as j inceases, it indicates that the overall imbalance can be controlled. However, if there is a break point at Step j, it implies that adding the  jth covariates greatly inceases WEIM, which is harmful to the control of the overall imbalance. Therefore, the algorithm should be stopped and print the outputs at Step j-1. Specifically, procedures to select the subset of covariates are given by the following algorithm.

	
	
	 Algorithm 2. Subset selection of covariates in the high dimensional case.

	For each j ‚àà{ 1,‚Ä¶, L},
	
		
    compute the estimated weights ≈µ_i^(j) using v_K2(ùêó) = (1,X_(1),‚Ä¶, X_(j));
		
     calculate WEIM^(j) in (8) using ≈µ_i^(j);
		
    add (j,WEIM^(j)) to the x-y plot and observe whether there is a break point at (j,WEIM^(j)):
		
			
    If no, let j=j+1;
			
    If yes, stop and output L_0 = j-1.
		
	
	The selected subset of the covariates is (X_(1),‚Ä¶, X_(L_0)).
	
	
	
	

 ¬ß.¬ß Causal effect estimation

	
	In this subsection, both parametric and nonparametric approaches are developed to estimate the causal effect function. A weighted optimization estimation is defined under the parametric framework and broadcasted nonparametric tensor regression method (<cit.>) is used to estimate the causal effect function under the nonparametric framework.
	

  ¬ß.¬ß.¬ß Parametric approach

	The causal effect function is parametrized as s(ùê≠;Œ≤), assume that it has a unique solution  Œ≤^* defined as
	
    Œ≤^* = agrmin_Œ≤‚à´_ùíØùîº[Y(t)- s(ùê≠;Œ≤) ]^2f_T(t)dt.

	
	The difficulty of solving Equation (12) is that the potential outcome Y(t) is not observed for all t. Hence, Proposition 1 is proposed to connect the potential outcome with the observed outcome. 

	
	Proposition 1  Under Assumption 1, it can be shown that
	
    ùîº[w(Y- s(ùê≠;Œ≤) )^2] = ‚à´_ùíØùîº[Y(t)- s(ùê≠;Œ≤) ]^2f_T(t)dt.

	The proof of Proposition 1 can be found in Appendix A.2. Note that Y(t) on the right hand side of Equation (13) represents the potential outcome and Y on the left hand side represents the observed outcome. Proposition 1 indicates that by having w on the left hand side of Equation (13), one can represent the objective function with the potential outcome (right side) by that with the observed outcome (left side). Therefore, the true value Œ≤^* is also a solution of the weighted optimization problem:
	
    Œ≤^* = argmin_Œ≤ùîº[w(Y- s(ùê≠;Œ≤))^2].

	This result implies that the true value Œ≤^* can be identified from the observational data.
	One can obtain the estimator based on the sample, which is
	
    Œ≤ÃÇ = argmin_Œ≤‚àë_i=1^n≈µ_ÃÇ√Æ(Y_i- s(ùêì_i;Œ≤) )^2.

	
	

  ¬ß.¬ß.¬ß Nonparametric approach

	Suppose ùîº(Y(ùê≠)) = s(ùê≠). In a similar manner to the proof of Proposition 1, it can be shown that 
	
    ùîº(wY |ùêì=t) = ùîº(Y(ùê≠)).

	Existing work of nonparametric tensor regression suffers from a slow rate of convergence due to the curse of dimensionality. Even if one flattens the tensor covariate into a vector and applies common nonparametric regression models such as additive models or single-index models to it, this issue still exists. Besides, when dealing with a vectorized tensor covariate, one would ignore the latent tensor structure and this might result in large bias. To meet these challenges, we adopt the broadcasted nonparametric tensor regression method (<cit.>) to estimate the causal effect function s(ùê≠).
	The main idea of the broadcasted nonparametric tensor regression is to use the (low-rank) tensor structure to discover important regions of the tensor so as to broadcast a nonparametric modeling on such regions. Specifically, assume that 
	
    s(ùêì) = c+1/pq‚àë_r=1^R<Œ≤_1^(r)‚àòŒ≤_2^(r), F_r(ùêì)>,

	where c‚àà R,  ùêì‚àà R^p √ó q,  F_r(ùêì) = ‚Ñ¨(f_r, ùêó), ‚Ñ¨ is a broadcasting operator, which is defined as
	
    (‚Ñ¨(f,ùêì))_i_1,i_2 = f(T_i_1,i_2),  for all i_1,i_2.

	The broadcasted functions f_r, r=1,‚Ä¶,R, will be approximated by B-spline functions, i.e.,
	
    f_r(x) ‚âà‚àë_d=1^DŒ±_r,db_d(x),

	where ùêõ(x) = (b_1(x),‚Ä¶,b_D(x))^' is a vector of B-spline basis functions and Œ±_r,d's are the corresponding spline coefficients. Define Œ±_r = (Œ±_r,1,‚Ä¶,Œ±_r,D)^' and (Œ¶(ùêì))_i_1,i_2,d = b_d(T_i_1,i_2), the regression function (16) can be approximated by 
	
    s(ùêì) ‚âà c+1/pq‚àë_i=1^R<Œ≤_1^(r)‚àòŒ≤_2^(r)‚àòŒ±_r, Œ¶(ùêì)>.

	To separate out the constant effect from f_r's, the condition ‚à´_0^1 f_r(x) dx=0 is imposed, which leads to 
	
    ‚à´_0^1‚àë_d=1^DŒ±_r,db_d(x)dx=0,  r=1,‚Ä¶, R.

	Then the following optimization problem is considered:
	
    argmin_c,ùêÜ ‚àë_i=1^n (≈µ_iY_i- c-1/pq<ùêÜ,Œ¶(ùêì_i)>)^2

	s.t.
	
    ùêÜ = ‚àë_r=1^RŒ≤_1^(r)‚àòŒ≤_2^(r)‚àòŒ±_r  
    ‚àë_d=1^DŒ±_r,d‚à´_0^1 b_d(x)dx =0,  r=1,‚Ä¶,R,

	and the estimated regression function is 
	
    ≈ù(ùêì) = ƒâ+1/pq <ùêÜÃÇ,Œ¶(ùêì)>,

	where (ƒâ,ùêÜÃÇ) is a solution of (20). 
	Since optimization problem (20) contains too many constraints, it is not computationally efficient to solve it directly. To further simplify the optimization problem, an equivalent truncated power basis (<cit.>) is used to reduce the constraints. Specifically, let bÃÉ_d(x), d=1,‚Ä¶,D denote the truncated basis:
	
    bÃÉ_1(x)=1,bÃÉ_2(x)=x,‚Ä¶, bÃÉ_œÇ(x) = x^œÇ-1,
    bÃÉ_œÇ+1(x) = (x-Œæ_2)_+^œÇ-1,‚Ä¶, bÃÉ_D(x)=(x-Œæ_D-œÇ+1)_+^œÇ-1,

	where œÇ and (Œæ_2,‚Ä¶,Œæ_D-œÇ+1) are the order and the interior knots of the aforementioned B-spline, respectively. Based on these basis functions, consider the following optimization
	
    argmin_cÃÉ,ùêÜÃÉ ‚àë_i=1^n (≈µ_iY_i- cÃÉ-1/pq<ùêÜÃÉ,Œ¶ÃÉ(ùêì_i)>)^2

	s.t.
	
    ùêÜÃÉ = ‚àë_r=1^RŒ≤_1^(r)‚àòŒ≤_2^(r)‚àòŒ±ÃÉ_r,

	where Œ¶ÃÉ(ùêì)_i_1,i_2,d = bÃÉ_d+1(ùêì_i_1,i_2), d=1,‚Ä¶,D and Œ±ÃÉ_r ‚àà R^D-1 is the vector of coefficients. Compared with (20), the mean zero constraints are removed by reducing one degree of freedom in the basis functions. According to <cit.>, Lemma B.1, one can show that
	
    ≈ù(ùêì) = ƒâÃÉÃÇ+1/pq <ùêÜÃÇÃÉÃÇ,Œ¶ÃÉ(ùêì)>,

	where (ƒâÃÉÃÇ,ùêÜÃÇÃÉÃÇ) is the solution of (22). 
	The optimization problem (22) can be solved by the scaled-adjusted block-wise descent algorithm (<cit.>).
	
	

¬ß THEORETICAL PROPERTIES

	In this section, the large sample properties of the proposed estimators in section 3 are established. First the consistency of the estimated weight in section 3.1 is shown, then the consistency of the parametric estimator in section 3.2.1 and the convergence rate of the nonparametric estimator in section 3.2.2 are shown. The following assumptions are made. 
	

	Assumption 4
	
	
	
		
		
		
		
		
  * There exists a constant c_0 such that 0 < c_0 < 1, and c_0 ‚â§exp (z-1) ‚â§ 1-c_0 for any z= MÃÉ_K(ùê≠,ùê±)^'Œ∏ with Œ∏‚ààint(Œò). Besides, exp (z-1) = O(1) in some neighborhood of z^* = MÃÉ_K(ùê≠,ùê±)^'Œ∏^*, where MÃÉ_K(ùê≠,ùê±)= Œõ (m_K(ùê≠,ùê±)-mÃÖ_K) and Œõ= diag(Œª_1,‚Ä¶,Œª_K).
		
  * There exists a constant C such that 
		E {MÃÉ_K(ùêì_i, ùêó_i)MÃÉ_K(ùêì_i, ùêó_i)^'}‚â§ C. 
		
  * Œ¥ = o(n).
		
  * sup_(ùêì,ùêó) exp{‚àë_j=1^KŒ∏_j^* Œª_j [m_K,j(ùêì,ùêó)-Em_K,j(ùêì,ùêó)] } = O(1).
	 
	
	
	Assumption 5
	
		
  * The parameter space Œò_1 is a compact set and the true parameter Œ≤_0 is in the interior of Œò_1.
		
  * (Y-s(T;Œ≤))^2 is continuous in Œ≤, ùîº[sup_Œ≤(Y-s(T;Œ≤))^2] < ‚àû and sup_Œ≤ùîº[(Y-s(T;Œ≤))^4]  < ‚àû.
	 
	
	
	Assumption  6
	
		
  * s(ùê≠;Œ≤) is twice continuously differentiable in Œ≤‚ààŒò_1 and let h(ùê≠;Œ≤) ‚â°‚ñΩ_Œ≤ s(ùê≠;Œ≤).
		
  * ùîº{ w(Y-s(ùêì;Œ≤))h(ùêì;Œ≤) } is differentiable with respect to Œ≤ and 

		U ‚â° - ‚ñΩ_Œ≤ùîº{ w(Y-s(ùêì;Œ≤))h(ùêì;Œ≤) }|_Œ≤=Œ≤^* is nonsingular.
		
  * ùîº[sup_Œ≤| Y-s(T;Œ≤) |^2+Œ¥] < ‚àû for some Œ¥ >0 and there extists some finite positive constants a and b such that ùîº[sup_Œ≤_1:  ||Œ≤_1-Œ≤||  < Œ¥_1| s(T;Œ≤_1) - s(T;Œ≤) |^2]^1/2 < a¬∑Œ¥_1^b for any Œ≤‚ààŒò_1 and any small Œ¥_1 >0.
	
	
	
	
	Assumption 7
	
		
  * The treatment ùêì‚àà [0,1]^p√ó q has a continuous probability density function f, which is bounded away from zero and infinity.
		
  * The vector of random errors, œµ = (œµ_1,‚Ä¶,œµ_n)^', has independent and identically distributed entries. Each œµ_i is sub-Gaussian with mean 0 and sub-Gaussian norm œÉ < ‚àû.
		
  * The true broadcasted functions f_0r‚àà‚Ñã, r= 1,‚Ä¶,R_0. Here ‚Ñã is the space of functions from [0,1] to R satisfying the H√∂lder condition of order œâ, i.e.,
		
    ‚Ñã = { g: | g^(l) (x_1)-g^(l) (x_2) |‚â§ S_1| x_1-x_2 |^œâ, ‚àÄ x_1,x_2 ‚àà [0,1] },

		for some constant S_1>0, where g^(l) is the l-th derivative of g, such that œâ‚àà (0,1] and œÑ = l+œâ >1/2.
		
  * The order of the B-spline used in (16) satisfies œÇ‚â•œÑ+1/2. Let 0= Œæ_1 < Œæ_2 <‚Ä¶ < Œæ_D-œÇ+2=1 denote the knots of B-spline basis and assume that
		
    h_n = max_d=1,‚Ä¶, D-œÇ+1|Œæ_d+1
    			-Œæ_d |‚âç D^-1 and h_n/min_d=1,‚Ä¶, D-œÇ+1|Œæ_d+1
    				-Œæ_d |‚â§ S_2

		for some constant S_2>0.
	 
	
	Assumption 4(1) enables consistency of Œ∏ÃÇ to translate into consistency of the weights.  Assumption 4(2) is a standard technical condition that restricts the magnitude of the basis functions; see also Assumption 4.1.6 of <cit.> and Assumption 2(2) of <cit.>. Assumption 4(3) requires that the threshold parameter Œ¥ should be much smaller than the sample size. Assumption 4 (4) is needed for consistency of the estimated weight. Assumption 5(1) is a commonly used assumption in nonparametric regression. Assumption 5(2) is an envelope condition applicable to the uniform law of large numbers. Assumption 6(1) and (2)  impose sufficient regularity conditions on the causal effect function and its derivative function. Assumption 6(3) is a stochastic equicontinuity condition, which is needed for establishing weak convergence (<cit.>). Assumption 7(1), (3) and (4) are common in nonparametric regression models. In particular, Assumption 7(3) and (4) regularize the space where the true broadcasted functions lie in and guarantee that they can be approximated welll by B-spline functions. Similar assumptions can be found in <cit.> and <cit.>. Assumption 7(2) is a standard tail condition of the error. Based on these assumptions, the following theorems are established. 

	

	
	Theorem 2. 	Let Œ∏ÃÇ denote the solution to Problem (10) and 
	
    ≈µ_i = exp{‚àë_k=1^KŒ∏ÃÇ_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k) -1} ,  i=1,‚Ä¶,n,

	then under Assumptions 1-4,

	
		
		
  * ‚à´|ùê∞ÃÇ-ùê∞^* |^2 dF(ùê≠,ùê±) = O_p(n^-1).
		
  * 1/n‚àë_i=1^n|≈µ_i- w_i^* |^2 = O_p(n^-1).
	
	Based on Theorem 2, we can establish the consistency of the parametric estimator and the convergence rate of the nonparametric estimator.

	
	Theorem 3  
	
		
  * Under Assumptions 1-5, ||Œ≤ÃÇ-Œ≤^* || ‚Üí_p 0. 
		
  * Under Assumptions 1-6, ‚àö(n)(Œ≤ÃÇ-Œ≤^*) ‚Üí_d N(0,V), where 
		
    V = 4U^-1¬∑ùîº{ w^2(Y-s(ùêì;Œ≤^*))^2h(ùêì;Œ≤^*)h(ùêì;Œ≤^*)^'}¬∑ U^-1

	
	
	Theorem 4  If Assumptions 1-4 and 7 hold, R‚â• R_0, and 
	
    n > S_1 h_n^-2-2/log(h_n)(log^-2(h_n))(R^3+R(p+q)+RD)

	for some large enough constant S_1>0, then 
	
	
	
	
	
	
	
	
    ||≈ù(ùêì)-s_0(ùêì) ||^2 = O_p  ( R^3+R(p+q)+RD/n )+O_p  ( {‚àë_r=1^R_0||vec(ùêÅ_0r)||_1/pq}^2 1/D^2œÑ ),

	where s_0(ùêì) = c_0+1/pq‚àë_r=1^R_0<Œ≤_1^(0r)‚àòŒ≤_2^(0r), F_0r(ùêì)> represents the true regression function.
	The proofs of Theorem 2, 3 and 4 can be found in Appendix A.3, A.4 and A.5, respectively. 
	
	
	

¬ß SIMULATION

	To evaluate the finite sample performance of the proposed method, simulation studies are carried out under different data settings. The main motivation of the simulation is to compare the proposed method with three other methods when the outcome model are linear and nonlinear in various ways.
	

 ¬ß.¬ß The low-dimensional covariate setting

	In this subsection, we compare the performance of the proposed method (WEBM) with the unweighted method (Unweighted), entropy balancing method (EB) and univariate approximate balancing method (MDABW) in the low-dimensional covariate setting, where EB refers to the method proposed by <cit.> that balances covariates exactly and MDABW refers to the method proposed by <cit.> that balances covariates approximately.
	Since the covariates are shared across all scenarios, their data generating process is first described. Specifically, we independently draw 5 covariates from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2, that is,
	
	
    ùêó = (X_1,....,X_5)^'‚àº  N_5(Œº,Œ£)  with Œº= [ 0; ‚ãÆ; 0 ]and Œ£=[     1   0.2     ‚Ä¶   0.2;   0.2     1 0.2 ‚Ä¶   0.2;     4;   0.2   0.2     ‚Ä¶     1 ]_5√ó5.

	

	
	Consider a linear treatment assignment model, which is defined as
	
    ùêì_i= X_i1ùêÅ_1+X_i2ùêÅ_2+X_i3ùêÅ_3+ùêÑ_i,

	where ùêÅ_j = [ 1 0; 0 1; 1 1 ]_3√ó2, j=1,2,3 denotes the jth coefficient matrix, and ùêÑ_i ‚àà R^3√ó 2 denotes the error matrix, whose element follows a standard normal distribution. For the outcome model, we consider four scenarios and conduct 100 Monte Carlo simulations for each scenario. The first two scenarios assume an outcome model that is linear in treatment and the others assume a nonlinear relationship.
	In scenario 1, the linear outcome model is defined as
	
    Y_i = 1+ <ùêÅ, ùêì_i>+X_i1+(X_i2+1)^2+X_i4^2+œµ_i,

	where ùêÅ = [ 1 0; 0 1; 1 1 ]_3√ó2 and œµ_i ‚àº N(0,2^2). In this scenario, v_k2(ùêó) = (1, ùêó, ùêó*ùêó)^', where * represents the Hadamard product of two matrices, and the corresponding element of ùêó*ùêó is (X*X)_ij = (x_ijx_ij).
	
	In scenario 2, the linear outcome model is defined as
	
    Y_i = 1+ <ùêÅ, ùêì_i>+X_i2+X_i3+X_i1X_i2+‚Ä¶+X_i4X_i5+œµ_i,

	where ùêÅ and œµ_i are the same as in Equation (21). In this scenario, the interaction terms are strong confounders, hence set v_k2(ùêó) = (1, ùêó, X_jX_k)^', 1‚â§ j < k ‚â§ 5.  
	
	In scenarios 3 and 4, the nonlinear outcome models are considered and are defined as
	
    Y_i = 1+ <ùêÅ, F_1(ùêì_i)>+X_i1+(X_i2+1)^2+X_i4^2+œµ_i,

	and
	
    Y_i = 1+ <ùêÅ, F_1(ùêì_i)>+X_i2+X_i3+X_i1X_i2+‚Ä¶+X_i4X_i5+œµ_i,

	respectively.
	Here ùêÅ and œµ_i are the same as in Equation (24), (F_1(ùêì))_k_1,k_2= f_1(T_k_1,k_2)= T_k_1,k_2+0.6sin{2œÄ(T_k_1,k_2-0.5)^2 }. For all four scenarios, consider u_K1(ùêì) =(1, vec(ùêì)^')^' for simplicity.
	For each method, the mean RMSE and its standard deviation of the coefficient estimates for the linear outcome model, and those of the fitted values for the nonlinear outcome model are reported based on 100 data replications. 
	
	Table 1 shows the mean RMSE and its standard deviation of the coefficient matrix for the linear outcome model. Observe that the mean RMSE of WEBM is the smallest among the four methods, and the standard deviation of WEBM is the second smallest while that of Unweighted is the smallest in both scenario 1 and scenario 2. Besides, the results of scenario 2 indicate that MDABW and EB  have poor performance when the basis function of covariates includes interaction entries, and their mean RMSEs are even larger than those of Unweighted. The mean RMSE and standard deviation of all methods decreases as the sample size increases.
	Table 2 shows the mean RMSE and its standard deviation of the fitted values of the nonlinear outcome model. As can be seen, the results for both scenario 3 and 4 are similar to those in Table 1, that is, WEBM performs the best with both the smallest mean RMSE in all cases. Similarly, MDABW and EB methods have poor performance when the basis function of covariates includes interaction entries and the mean RMSE and standard deviation of all methods decreases as the sample size increases.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

 ¬ß.¬ß The high-dimensional covariate setting

	In this subsection, since the MDABW and EB methods can only deal with the low-dimensional covariate case, we compare the performance of the proposed method (WEBM) with the Unweighted method and the method (Mapping) proposed by <cit.>, which selects a small important subset of covariates by a joint screening procedure, 
	in the high-dimensional covariate setting. Since the Mapping method can only deal with linear outcome model, these two methods are compared in the linear outcome model setting.
	For both methods, set the sample size n=500 and the dimension of ùêì to be 3 √ó 2. Consider two scenarios (scenario 5-6) with the dimension of covariates  L=49 and L=99, respectively. The motivation for such settings is to consider the number of constraints K < n (scenario 5) and K>n (scenario 6), respectively. For the basis functions, u_K1(ùêì)= (1,vec(ùêì)^')^' and v_K2(ùêó) = (1,ùêó^')^' are considered.
	Additionally, covariates are drawn from a multivariate normal distribution with mean 0, variance 1 and covariance 0.2. For each scenario, consider a linear treatment assignment model, which is defined as
	
    ùêì_i= X_i1ùêÅ_1+X_i2ùêÅ_2+‚Ä¶+X_i5ùêÅ_5+ùêÑ_i,

	where ùêÅ_j = [ 1 0; 0 1; 1 1 ]_3√ó2, j=1,2,3,4,5 denotes the jth coefficient matrix, and ùêÑ_i ‚àà R^3√ó 2 denotes the error matrix, whose element follows a standard normal distribution. 
	Moreover, the linear outcome model is defined as
	
    Y_i = 1+ <ùêÅ, ùêì_i>+X_i1+X_i2+‚Ä¶+X_i5+œµ_i,

	
	
	
	
	
	where ùêÅ=[ 1 0; 0 1; 1 1 ]_3√ó2, œµ_i ‚àº N(0,2^2).
	For each method, the mean RMSE and its standard deviation of the tensor regression estimation for the linear outcome model are reported based on 100 data replications.  
	Table 3 shows the mean RMSE and its standard deviation of coefficent matrix for the linear outcome model in the high-dimensional covariate setting. The results indicate that WEBM performs the best with the smallest mean RMSE in all cases.
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	
	

¬ß APPLICATION

	Intelligence Quotient (IQ) is based on biological attributes, most of which are inherited from parents. It mainly refers to a person's cognitive abilities, such as logical reasoning, pattern recognition, and short-term memory. Of course, due to genetic mutations, parents with average IQ may have offspring with superior intelligence, and vice versa. But IQ also has social attributes. Studies have found that IQ has obvious plasticity, and environmental factors affect IQ levels. From 1947 to 2002, the IQ level of developed countries rose steadily at a rate of 3% every 10 years, which is called the "Flynn effect." This effect has been repeatedly observed in various countries, various age groups, and a large number of different environments, and has become a reliable evidence that "environment affects IQ" (<cit.>). In particular, literatures have shown that taking training lessons, such as music, sports, chess and so on, can significantly enhance children's IQ (<cit.>; <cit.>; <cit.>; <cit.>). 
	Unfortunately, there are some limitations of existing studies. First, existing studies have only analyzed the relevant effects of attending single training course at a fixed age on children's IQ. In practice, however, a child may attend different training courses at the same age and participation may also vary by age. Second, existing studies mainly investigated the impact of whether or not to attend training classes, ignoring the effect of class duration. Third, existing studies only analyzed the correlation between children's participation in training courses on children's IQ, while their causal relationship is much more of a concern. To overcome the aforementioned limitations, the proposed method is applied to a matrix treatment, which contains the structural information of children's participation in training courses, to investigate their causal impact on children's IQ.
	The data are obtained from the Brain Science Innovation Institute of East China Normal University's Child Brain Intelligence Enhancement Project, whose goal is to explore brain development and help improve brain power. The treatment we are interested in is a 2 √ó 5 matrix about children's participation in training courses, whose rows represent age groups, including 3-6 years old and 6-9 years old, columns represent the types of training courses, including knowledge education (Chinese, mathematics and English, etc.), art (music, art, calligraphy, etc.), sports (swimming, ball games, etc.), hands-on practice (STEM, Lego, etc.) and thinking training (logical thinking, EQ education, attention, etc.), and each element of the matrix represents the number of hours of class per week. The outcome is children's IQ and the pre-treatment covariates include children's gender as well as parental education, which have been shown to be associated with both the treatment and outcome variables (<cit.>; <cit.>; <cit.>; <cit.>). A complete-case analysis is conducted with a sample of 103 participants.  
	Before estimating the causal effect, we first examine the covariate balancing of WEBM, MDABW and EB methods based on the WEIM statistics. The statistic WEIM defined in equation (8) is 0.1050 for the WEBM method, 0.3761 for the MDABW method and 0.9881 for the EB method, which implies that WEBM balances covariates well while EB does not. Assume a linear tensor outcome model and the bootstrap method with 200 replicates is used to obtain confidence intervals for the parameter estimates. The results are shown in Table 4.
	
		
		
		
	
	Table 4 shows the estimated causal effects of the duration of attending different classes at different ages on children's IQ. It can be seen that most methods (except EB) suggest that the duration of attending hands-on practice courses at 6-9 years old has a siginificantly positive impact on children's IQ. This finding is not only consistent with previous findings that participation in hands-on practice classes can improve children's IQ (<cit.>; <cit.>; <cit.>),  but further suggests that longer participation in hands-on practice classes is more beneficial to children's IQ.
	
	The results imply that future work of an intervention study about attending hands-on practice training courses, which in turn may improve children's IQ, is suggested. Besides, the width of confidence interval of the estimated causal effect based on WEBM is the smallest, which implies that the estimation accuracy of WEBM is the highest. 
	
	
	
	
	

¬ß CONCLUSION AND DISCUSSION
	
	In this study, the weighted Euclidean balancing method is proposed, which obtains stabilized weights by adopting a single measure that represents the overall imbalance. An algorithm for the high-dimensional covariate setting is also proposed. Furthermore, parametric and nonparametric methods are developed to estimate the causal effect and their theoretical properties are provided. The simulation results show that the proposed method balances covariates well and produces a smaller mean RMSE compared to other methods under variaous scenarios. In the real data analysis, the WEBM method is applied to investigate causal effect of children's participation in training courses on their IQ. The results show that the duration of attending hands-on practice at 6-9 years old has a siginificantly positive impact on children's IQ.
	Since the causal effect function ùîº(Y(t)) is more general, we mainly consider it as the estimand for matrix treatment in this paper. Actually, one can also consider the average treatment effect  (ùîº(Y(t+ t)-Y(t))) or average partial effect (ùîºY(t+ t)-ùîºY(t)/ t)  , which can be easily estimated based on the estimates of causal effect function  (<cit.>). Indeed, the causal effect function provides a complete description of the causal effect, rather than a summary measure. Moreover, parametric and nonparametric methods are developed to estimate the causal effect function. Parametric method is recommended when reasonable assumptions can be made about the true model since it is easier to implement and requires less sample size. Despite nonparametric method has higher requirements of the sample size, one can choose to use it according to the real situations due to its higher flexibility. Besides, this paper mainly focuses on the small-scale matrix treatment. Large-scale matrix treatment with low-rank structure can also be considered. In such case, one may control the overall imbalance by only balancing their non-zero elements based on some decomposition technology, and this will be investigated in future work.
	
	
	
	*
	apalike
	
	
	

¬ß APPENDIX

	

 ¬ß.¬ß A.1.Proof of Theorem 1
 
	The primal problem is
	
    min_ùê∞‚àë_i=1^nw_ilog(w_i)

	s.t.
	
    ‚àë_j=1^K{Œª_j^2 [‚àë_i=1^n w_i (m_K,j(ùêì_i,ùêó_i)-mÃÖ_K,j)]^2 }‚â§Œ¥.

	
	Let ||Œ∏||_2 = ‚àö(Œ∏_1^2+‚Ä¶+Œ∏_K^2) be the l_2 norm for an arbitrary K-dimensional vector Œ∏ =(Œ∏_1,‚Ä¶,Œ∏_K)^' and Œõ = diag(Œª_1,‚Ä¶,Œª_K), then the inequality constraint in the primal problem can be rewritten as ||‚àë_i=1^n w_i Œõ (m_K(ùêì_i,ùêó_i)-mÃÖ_K) ||_2 ‚â§‚àö(Œ¥). Let ùíú‚äÜ R^K be a convex set such that ùíú = { a ‚àà R^K: || a||_2 ‚â§‚àö(Œ¥)}. Define I_ùíú(a) = 0 if a ‚ààùíú and I_ùíú(a) = ‚àû otherwise. Then, the primal problem (15) is equivalent to the following optimaization problem:
	
    min_ùê∞ ‚àë_i=1^nw_ilog(w_i)+I_ùíú( ‚àë_i=1^n w_i Œõ (m_K(ùêì_i,ùêó_i)-mÃÖ_K)).
 
	Let h(w) = ‚àë_i=1^n w_i log(w_i), the conjugate function of h is 
	
    h^*(w)    = sup_t(‚àë_i=1^n w_i t_i-‚àë_i=1^n w_i log(w_i)) 
       = sup_t ‚àë_i=1^n (w_i t_i-w_i log(w_i)) 
       = ‚àë_i=1^n sup_t_i(w_i t_i-w_i log(w_i)) 
       =  ‚àë_i=1^n f^*(w_i),

	where f^*(w_i) = sup_t_i(w_i t_i-w_i log(w_i)) is the conjugate function of f(w_i) = w_i log(w_i). Let g(Œ∏) = I_ùíú(Œ∏) for any Œ∏‚àà R^K, then the conjugate function of g is 
	
    g^*(Œ∏)    = sup_a (‚àë_k=1^KŒ∏_ka_k -T_ùíú(a) ) 
       = sup_|| a ||_2 ‚â§‚àö(Œ¥) (‚àë_k=1^KŒ∏_ka_k) 
       = sup_|| a ||_2 ‚â§‚àö(Œ¥)  (||Œ∏||_2 || a ||_2) 
       = ‚àö(Œ¥)||Œ∏||_2.

	Define the mapping H: R^n ‚Üí R^K such that Hw = ‚àë_i=1^n w_i Œõ (m_K(ùêì_i,ùêó_i)-mÃÖ_K), then H is a bounded linear map. Let H^* be the adjoint operator of H, then for all Œ∏ = (Œ∏_1,‚Ä¶,Œ∏_K)^'‚àà R^K,
	
    H^*Œ∏ = (‚àë_k=1^KŒ∏_k Œª_k (m_K,k(ùêì_1,ùêó_1)-mÃÖ_K,k), ‚Ä¶, ‚àë_k=1^KŒ∏_k Œª_k (m_K,k(ùêì_n,ùêó_n)-mÃÖ_K,k) )^'.

	Define Œ∏ÃÉ = HwÃÉ = 1/n^r‚àë_i=1^nŒõ (m_K(ùêì_i,ùêó_i)-mÃÖ_K), where wÃÉ = (1/n^r,‚Ä¶,1/n^r)^'‚àà dom(F). Here, we choose b to be sufficiently large such that ||Œ∏ÃÉ||_2 ‚â§‚àö(Œ¥), then we obtain that g(Œ∏ÃÉ) = 0 and g is continuous at Œ∏ÃÉ. Therefore, Œ∏ÃÉ‚àà H(dom(F) ‚à© cont(g)), which implies that H(dom(F) ‚à© cont(g)) ‚â†‚àÖ. Here, dom(F) and cont(g) denotes the domain of F and the continuous set of g, respectively. Therefore, the strong duality condition of the Fenchel duality theorem is verified. Moreover,
	
    F(H^*Œ∏)+g^*(-Œ∏) = ‚àë_i=1^n f^*(‚àë_k=1^KŒ∏_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k))+‚àö(Œ¥)||Œ∏||_2.

	According to the Fenchel duality theorem (Mohri et al. (2018), Theorem B.39), we have 
	
    min_w  ‚àë_i=1^nw_ilog(w_i)+I_ùíú( ‚àë_i=1^n w_i Œõ (m_K(ùêì_i,ùêó_i)-mÃÖ_K)) 
       = min_w  ‚àë_i=1^n f^*(‚àë_k=1^KŒ∏_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k))+‚àö(Œ¥)||Œ∏||_2.
 
	Furthermore, since the strong duality condition holds, we can conclude that H^*Œ∏ÃÇ is a subgradient of F at ≈µ. That is, 
	
    ‚àë_k=1^KŒ∏ÃÇ_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k) = log(≈µ_i)+1.

	Therefore, ≈µ_i = exp(‚àë_k=1^KŒ∏ÃÇ_k Œª_k (m_K,k(ùêì_i,ùêó_i)-mÃÖ_K,k)-1). The proof of theorem 1 is completed.
	
	
	

 ¬ß.¬ß A.2.Proof of Proposition 1

	Using the law of total expectation and Assumption 1, we can deduce that
	
    ùîº[w(Y- s(ùêì;Œ≤) )^2]
       =E[f(T)/f(T|X)(Y-s(ùêì;Œ≤)  )^2]
       = ùîº(ùîº[f(T)/f(T|X)(Y- s(ùêì;Œ≤)  )^2] |T=t,X=x )
       = ùîº(f(t)/f(t|x)ùîº([(Y- s(ùêì;Œ≤)  )^2] |T=t,X=x) )
       = ‚à´_ùíØ√óùí≥f(t)/f(t|x)ùîº[(Y(T)- s(ùêì;Œ≤)  )^2 |T = t, X= x]f(t|x)dtdx
       =‚à´_ùíØ√óùí≥ùîº[(Y(T)- s(ùêì;Œ≤)  )^2 |T = t, X= x]f(t)f(x)dtdx
       = ‚à´_ùíØ√óùí≥ùîº[(Y(t)- s(ùêì;Œ≤) )^2 |X= x]f(t)f(x)dtdx   (using Assumption 1)
       = ‚à´_ùíØùîº[(Y(t)- s(ùêì;Œ≤)  )^2] f(t)dt  .

	Hence, we complete the proof of Proposition 1.
	
	

 ¬ß.¬ß A.3.Proof of Theorem 2

	The first order optimality condition for the dual probblem (10) is 
	
    ‚àë_i=1^nexp{‚àë_j=1^KŒ∏ÃÇ_jŒª_jM_K,j(ùêì_i,ùêó_i) }¬∑Œª_jM_K,j(ùêì_i,ùêó_i) +‚àö(Œ¥)Œ∏ÃÇ_j/||Œ∏ÃÇ||_2 =0,   j=1,‚Ä¶,K,

	where M_K,j(ùêì_i,ùêó_i) = m_K,j(ùêì_i,ùêó_i)- mÃÖ_K,j, M_K(ùêì_i,ùêó_i)= (M_K,1(ùêì_i,ùêó_i), ‚Ä¶, M_K,K(ùêì_i,ùêó_i))^'.
	Let Œõ = diag(Œª_1,‚Ä¶,Œª_K) and 
	
    1/n‚àë_i=1^nŒ¶(ùêì_i,ùêó_i;Œ∏) = 1/n‚àë_i=1^nexp{‚àë_j=1^KŒ∏_jŒª_j[m_K,j(ùêì_i,ùêó_i)-ùîº(m_K,j)] }Œõ [m_K(ùêì_i,ùêó_i) -ùîº(m_K)],

	which is a set of K estimating functions. Note that
	
    |ùîº( Œ¶(ùêì_i,ùêó_i;Œ∏^*)) |
       = |ùîº{exp{‚àë_j=1^KŒ∏_j^*Œª_j[m_K,j(ùêì_i,ùêó_i)-Em_K,j] }Œõ [m_K(ùêì_i,ùêó_i) -Em_K] }|
       ‚â§sup_(ùêì_i,ùêó_i) exp{‚àë_j=1^KŒ∏_j^*Œª_j[m_K,j(ùêì_i,ùêó_i)-Em_K,j] }¬∑|ùîºŒõ [m_K(ùêì_i,ùêó_i) -Em_K] |
       ‚â§ O(1) ¬∑0 = 0,

	hence we have ùîº( Œ¶(ùêì_i,ùêó_i;Œ∏^*))=0, which implies that Œ∏^* is the unique solution of ùîº( Œ¶(ùêì_i,ùêó_i;Œ∏))=0. Therefore, by the estimating equation theory (Van der Vaart (2000)), the solution of the estimating equations 
	
    1/n‚àë_i=1^nŒ¶(ùêì_i,ùêó_i;Œ∏) = 0,
 
	denoted by Œ∏ÃÉ, is asymptotically consistent for Œ∏^*. Furthermore, by the Taylor expansion, we have
	
    ‚àö(n)(Œ∏ÃÉ - Œ∏^*) ‚Üí_d N(0, Œ£),

	where Œ£ = { E(‚àÇŒ¶/‚àÇŒ∏^') }^-1 E(Œ¶Œ¶^'){ E(‚àÇŒ¶/‚àÇŒ∏) }^-1.
	Moreover, by the assumption that Œ¥ = o(n), we have 1/n‚àö(Œ¥)Œ∏_j/||Œ∏||_2 = o_p(n^-1/2) for any Œ∏‚àà int(Œò). Therefore, by the Slutsky's theorem, we obtain that 
	
    ‚àö(n)(Œ∏ÃÇ - Œ∏^*) ‚Üí_d N(0, Œ£).

	Let MÃÉ_K,j(ùêì_i,ùêó_i) = Œª_j(m_K,j(ùêì_i,ùêó_i)- mÃÖ_K,j),
	then 
	
    ≈µ_i    = exp{‚àë_j=1^KŒ∏ÃÇ_j Œª_j (m_K,j(ùêì_i,ùêó_i)-mÃÖ_K,j) -1}
       = exp{MÃÉ_K(ùêì_i,ùêó_i)^'Œ∏ÃÇ -1}

	
	
	
	
	
	
	
	
	
	
	
	
	
	
	By Mean Value Theorem, we can deduce that
	
    ‚à´|≈µ-w^*|^2dF(ùê≠,ùê±) 
       ‚â§sup_(ùê≠,ùê±) |exp{MÃÉ_K(ùê≠,ùê±)^'Œ∏_1 -1}|^2 √ó‚à´|MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*) |^2 dF(ùê≠,ùê±) 
       ‚â§ O_p(1) ¬∑‚à´|MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*) |^2 dF(ùê≠,ùê±),

	where Œ∏_1 lies between Œ∏ÃÇ and Œ∏^*.
	Since 
	
    ‚à´|MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*) |^2 dF(ùê≠,ùê±) 
        	= ‚à´MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*)(Œ∏ÃÇ-Œ∏^*)^'MÃÉ_K(ùê≠,ùê±) dF(ùê≠,ùê±)
       = tr{ (Œ∏ÃÇ-Œ∏^*)(Œ∏ÃÇ-Œ∏^*)^'‚à´MÃÉ_K(ùê≠,ùê±)MÃÉ_K(ùê≠,ùê±)^'dF(ùê≠,ùê±) }
       ‚â§ C tr{ (Œ∏ÃÇ-Œ∏^*)(Œ∏ÃÇ-Œ∏^*)^'}
       = C||Œ∏ÃÇ-Œ∏^*||^2 
       = O_p(n^-1).

	Then we have 	
	
    ‚à´|≈µ-w^*|^2dF(ùê≠,ùê±) = O_p(n^-1).

	Furthermore, one can show that 
	
    1/n‚àë_i=1^n|MÃÉ_K(ùê≠,ùê±)^'(Œ¥ÃÇ-Œ¥^*) |^2 - ‚à´|MÃÉ_K(ùê≠,ùê±)^'(Œ¥ÃÇ-Œ¥^*) |^2 dF(ùê≠,ùê±)=o_p(1).

	Hence,
	
    1/n‚àë_i=1^n|≈µ_i- w_i^*|^2 
       ‚â§sup_(ùê≠,ùê±) |exp{MÃÉ_K(ùê≠,ùê±)^'Œ∏_1 -1}|^2 ¬∑1/n‚àë_i=1^n|MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*) |^2 
       ‚â§ O_p(1) ‚à´|MÃÉ_K(ùê≠,ùê±)^'(Œ∏ÃÇ-Œ∏^*) |^2 dF(ùê≠,ùê±) +o_p(1) 
       = O_p(n^-1)

	Therefore, the proof of Theorem 2 is completed.
	
	

  ¬ß.¬ß.¬ß A.4.Proof of Theorem 3

	
	
	We first show that the conclusion of Theorem 3(1). 

	
	Since Œ≤ÃÇ (as a estimator of Œ≤^*) is a unique minimizer of 1/n‚àë_i=1^n≈µ_i(Y_i-s(T_i;Œ≤))^2(regarding ùîº[w(Y-s(T;Œ≤))^2], according to the theory of M-estimation (van der Vaart, 2000, Theorem 5.7), if
	
    sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n≈µ_ÃÇ√Æ(Y_i-s(T_i;Œ≤))^2-ùîº[w(Y-s(T;Œ≤))^2]) |‚Üí_p 0,

	then Œ≤ÃÇ‚Üí_p Œ≤^*.
	Note that
	
    sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n≈µ_ÃÇ√Æ(Y_i-s(T_i;Œ≤))^2-ùîº[w(Y-s(T;Œ≤))^2]) |
    ‚â§sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)(Y_i-s(T_i;Œ≤))^2 |
    
    		+sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^nw_i(Y_i-s(T_i;Œ≤))^2-ùîº[w(Y-s(T;Œ≤))^2]) |.

	We first show that sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)(Y_i-s(T_i;Œ≤))^2 | is o_p(1). Using the Causchy-Schwarz inequality and the fact that ≈µ‚Üí^L^2 w, we have
	
    sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)(Y_i-s(T_i;Œ≤))^2 |   ‚â§{1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)^2 }^1/2sup_Œ≤‚ààŒò_1{1/n‚àë_i=1^n(Y_i-s(T_i;Œ≤))^2 }^1/2
       ‚â§ o_p(1){sup_Œ≤‚ààŒò_1ùîº[w(Y-s(T;Œ≤))^2]+o_p(1) }^1/2
       =o_p(1).

	Thereafter, under Assumption 5, we can conclude that sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^nw_i(Y_i-s(T_i;Œ≤))^2-ùîº[w(Y-s(T;Œ≤))^2]) | is also o_p(1) (Newey and McFadden (1994), Lemma 2.4). Hence, we complete the proof for Theorem 3(1). Next, we give the proof of Theorem 3(2). Define
	
    Œ≤ÃÇ^* = argmin_Œ≤‚àë_i=1^n w_i(Y_i-s(ùêì_i;Œ≤))^2.

	Assume that 1/n‚àë_i=1^n w_i(Y_i - s(ùêì_i;Œ≤ÃÇ^*))h(ùêì_i;Œ≤ÃÇ^*)) = o_p(n^-1/2) holds with probablility to one as n ‚Üí‚àû
	
	By Assumption 5 and the uniform law of large number, one can get that
	
    1/n‚àë_i=1^n w_i(Y_i-s(T_i;Œ≤))^2 ‚Üíùîº{ w(Y-s(T;Œ≤))^2 } in probability uniformly over  Œ≤,

	which implies ||Œ≤ÃÇ^* -Œ≤^* ||‚Üí_p 0. Let
	
    r(Œ≤) = 2ùîº{ w(Y-s(T;Œ≤))h(T;Œ≤) },

	which is a differentiable function in Œ≤ and r(Œ≤^*) = 0. By mean value theorem, we have
	
    ‚àö(n)r(Œ≤ÃÇ^*)- ‚ñΩ_Œ≤ r(Œ∂) ¬∑‚àö(()n)(Œ≤ÃÇ^* - Œ≤^*) =‚àö(n)r(Œ≤^*) =0

	where Œ∂ lies on the line joining Œ≤ÃÇ^* and Œ≤^*. Since ‚ñΩ_Œ≤ r(Œ≤) is continuous at Œ≤^* and ||Œ≤ÃÇ^* -Œ≤^* ||‚Üí_p 0, then
	
    ‚àö(n)(Œ≤ÃÇ^* - Œ≤^*)  = ‚ñΩ_Œ≤ r(Œ≤^*)^-1¬∑‚àö(n) r(Œ≤ÃÇ^*) +o_p(1)

	Define the empirical process
	
    G_n(Œ≤)= 2/‚àö(n)‚àë_i=1^n{ w_i(Y_i-s(T_i;Œ≤))h(T_i;Œ≤) - ùîº{ w(Y-s(T;Œ≤))h(T;Œ≤)  }}.

	Then we have
	
    ‚àö(n)(Œ≤ÃÇ^* - Œ≤^*)
       =  ‚ñΩ_Œ≤ r(Œ≤^*)^-1¬∑{‚àö(n) r(Œ≤ÃÇ^*) -  2/‚àö(n)‚àë_i=1^n{ w_i(Y_i-s(T_i;Œ≤ÃÇ^*))h(T_i;Œ≤ÃÇ^*)  + 2/‚àö(n)‚àë_i=1^n{ w_i(Y_i-s(T_i;Œ≤ÃÇ^*))h(T_i;Œ≤ÃÇ^*)  }
       =  -‚ñΩ_Œ≤ r(Œ≤^*)^-1¬∑ G_n(Œ≤ÃÇ^*)+o_p(1)
       = U^-1¬∑{ G_n(Œ≤ÃÇ^*)-G_n(Œ≤^*) +G_n(Œ≤^*) } +o_p(1).

	By Assumption 5, 6, Theorem 4 and 5 of Andrews(1994), we have G_n(Œ≤ÃÇ^*)-G_n(Œ≤^*) ‚Üí_p 0. Thus,
	
    ‚àö(n)(Œ≤ÃÇ^* - Œ≤^*) = U^-12/‚àö(n)‚àë_i=1^n{ w_i(Y_i-s(T_i;Œ≤^*))h(T_i;Œ≤^*) } +o_p(1),

	then we can get that the asymptotic variance of ‚àö(n)(Œ≤ÃÇ^* - Œ≤^*) is V.
	Therefore, ‚àö(n)(Œ≤ÃÇ^* - Œ≤^*) ‚Üí_d N(0,V). Next, we will prove Œ≤ÃÇ‚Üí_p Œ≤ÃÇ^*.
	Since
	
	
    sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n≈µ_ÃÇ√Æ(Y_i-s(T_i;Œ≤))^2-1/n‚àë_i=1^nw_i(Y_i-s(T_i;Œ≤))^2) |
    ‚â§sup_Œ≤‚ààŒò_1|1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)(Y_i-s(T_i;Œ≤))^2 |
    ‚â§{1/n‚àë_i=1^n(≈µ_ÃÇ√Æ-w_i)^2 }^1/2sup_Œ≤‚ààŒò_1{1/n‚àë_i=1^n(Y_i-s(T_i;Œ≤))^2 }^1/2
    ‚â§ o_p(1){sup_Œ≤‚ààŒò_1ùîº[w(Y-s(T;Œ≤))^2]+o_p(1) }^1/2
    
    		=o_p(1),

	which implies Œ≤ÃÇ^* ‚Üí_p Œ≤ÃÇ. Then by Slutskey's Theorem, we can draw the conclusion that ‚àö(n)(Œ≤ÃÇ - Œ≤^*) ‚Üí_d N(0,V). Therefore, we have completed the proof of Theorem 3. 
	
	

  ¬ß.¬ß.¬ß A.5.Proof of Theorem 4

	For convenience, we use a mapping Œ©: R^p√ó q √ó D√ó R ‚Üí R^p√ó q √ó D to represent the operator of absorbing the constant into the coefficients of B-spline basis for the first predictor. More precisely, Œ© is defined by 
	
    ùêÜ^b= Œ©(ùêÜ,c),

	where ùêÜ_i_1,i_2,d= ùêÜ_i_1,i_2,d for (i_1,i_2) ‚â† (1,1) and ùêÜ_1,1,d=ùêÜ_1,1,d+pqc, d=1,‚Ä¶,D. It then follows from the property of B-spline functions that
	
    c+1/pq<ùêÜ,Œ¶(ùêì)> = 1/pq <ùêÜ^b,Œ¶(ùêì)>.

	We also write ùêÜ_0= ‚àë_r=1^R_0ùêÅ_0r‚àòŒ±_0r, r= 1,‚Ä¶, R_0. Suppose ùêÜÃÇ,ƒâ) is a solution to (19) and 
	
    ùêÜÃÇ = ‚àë_r=1^RŒ≤ÃÇ_1^(r)‚àòŒ≤ÃÇ_2^(r)‚àòŒ±ÃÇ_r,

	then by <cit.>, Lemma B.1, there exists ƒç‚àà R and 
	
    ùêÜÃå = ‚àë_r=1^RŒ≤ÃÇ_1^(r)‚àòŒ≤ÃÇ_2^(r)‚àòŒ±Ãå_r,

	such that
	
    ƒç+1/pq<ùêÜÃå ,Œ¶(ùêì)> = ƒâ+1/pq<ùêÜÃÇ ,Œ¶ÃÉ(ùêì)>,

	where Œ±Ãå_r= (Œ±Ãå_r,1,‚Ä¶,Œ±Ãå_r,D )^' satisfying
	
    ‚àë_d=1^DŒ±Ãå_r,du_d=0

	with u_d= ‚à´_0^1 b_d(x)dx.
	Using (27), 
	we have
	
    ‚àë_i=1^n (≈µ_iy_i-ƒç-1/pq<ùêÜÃå,Œ¶(ùêì_i)>)^2 ‚â§‚àë_i=1^n (≈µ_iy_i-c_0-1/pq<ùêÜ_0,Œ¶(ùêì_i)>)^2.
 
	Let ùêÜÃå^b= Œ©(Œ±Ãå,ƒç) and ùêÜ_0^b = Œ©(ùêÜ_0,c_0), then 
	
    ‚àë_i=1^n (≈µ_iy_i-1/pq<ùêÜÃå^b,Œ¶(ùêì_i)>)^2 ‚â§‚àë_i=1^n (≈µ_iy_i-1/pq<ùêÜ_0^b,Œ¶(ùêì_i)>)^2.

	Therefore, we have
	
    ‚àë_i=1^n ((≈µ_i-w_i+w_i)y_i-1/pq<ùêÜÃå^b,Œ¶(ùêì_i)>)^2 ‚â§‚àë_i=1^n ((≈µ_i-w_i+w_i)y_i-1/pq<ùêÜ_0^b,Œ¶(ùêì_i)>)^2,

	which leads to 
	
    ‚àë_i=1^n (w_iy_i-1/pq<ùêÜÃå^b,Œ¶(ùêì_i)>)^2    ‚â§‚àë_i=1^n (w_iy_i-1/pq<ùêÜ_0^b,Œ¶(ùêì_i)>)^2 
       +2 ‚àë_i=1^n (≈µ_i-w_i)y_i(1/pq<ùêÜÃå^b-ùêÜ_0,Œ¶(ùêì_i)).

	Let ùêÜ^# =ùêÜÃå^b-ùêÜ_0^b,  ùêö^# =vec(ùêÜ^# ),  ùêö_0^b =vec(ùêÜ_0^b), ùêöÃå^b= vec(ùêÜÃå^b) and ùêô
	= (ùê≥_1,‚Ä¶,ùê≥_n)^'‚àà R^n√ó pqD, where ùê≥_i = vec(Œ¶(ùêì_i)),i=1,‚Ä¶,n. 
	Let y_≈µ= (≈µ_1y_1,‚Ä¶, ≈µ_ny_n)^' and y_w = (w_1y_1,‚Ä¶, w_ny_n)^', then using (31) and working out the squares, we obtain
	
    1/p^2q^2||ùêôùêö^#||^2   ‚â§  2<1/pqùêôùêöÃå^b,y_w>-2<	1/pqùêôùêö_0^b,y_w>
       -21/p^2q^2<ùêôùêö^#,ùêôùêö_0^b>+2<1/pqùêôùêö^#, y_≈µ-y_w>  
       = 2<1/pqùêôùêö^#,y_≈µ-y_w>+2<1/pqùêôùêö^#,œµ>+2<	1/pqùêôùêö^#,y_w-œµ-1/pqùêôùêö_0^b>

	First, we show the upper bound of <1/pqùêôùêö^#,y_≈µ-y_w>. Using the Cauchy-Schwarz inequality, we have
	
    <1/pqùêôùêö^#,y_≈µ-y_w> 
       ‚â§||≈µ-w ||_2 ¬∑||1/pqùêôùêö^#||_2 
       ‚â§C_1‚àö(nh_n)/pq¬∑||≈µ-w ||_2 ¬∑||ùêö^#||_2

	
	By the conclusion of Theorem 2(2), we have 
	
    ||≈µ-w ||_2 = O_p(1).

	Applying (37) to (36), we can obtain that
	
    <1/pqùêôùêö^#,y_≈µ-y_w> ‚â§C_2‚àö(nh_n)/pq||ùêö^#||_2.

	Second, by the conclusion of <cit.>, (A.18) and (A.20), we can obtain the upper bound of
	<1/pqùêôùêö^#,œµ> and <	1/pqùêôùêö^#,y_w-œµ-1/pqùêôùêö_0^b>, which are
	
    <1/pqùêôùêö^#,œµ> ‚â§C_3/pq||ùêö^#||_2 { nh_n(R^3+R(p+q)+RD)}^1/2.

	and
	
    <1/pqùêôùêö^#,y_w-œµ-1/pqùêôùêö_0^b> ‚â§C_4/pq||ùêö^#||_2 {‚àë_r=1^R_0||vec(ùêÅ_0r)||_1/pq}n‚àö(h_n)/D^œÑ

	Therefore, applying (38), (39) and (40) to (35), we have
	
    C_5/pq||ùêö^#||_2^2 ‚â§ R_1 ||ùêö^#||_2,

	where  R_1 = C_6‚àö(D/n)+C_7  {D(R^3+R(p+q)+RD)/n}^1/2+C_8{‚àë_r=1^R_0||vec(ùêÅ_0r)||_1/pq}1/D^œÑ-1/2.
	
	By solving the second order inequality (41), we have
	
    C_5/pq||ùêö^#||_2 ‚â§ R_1

	Further, by Assumption 6 and <cit.>, (A.38) of Lemma A.2, we have
	
    ||≈ù(ùêì) -s(ùêì) ||^2   ‚â§ C_9 h_n 1/p^2q^2||ùêö^#||^2 
       = C_10R_1^2/D
       = O_P(1/n)+O_p(R^3+R(p+q)+RD/n)+O_p({‚àë_r=1^R_0||vec(ùêÅ_0r)||_1/pq}^2 1/D^2œÑ) 
       =O_p(R^3+R(p+q)+RD/n)+O_p({‚àë_r=1^R_0||vec(ùêÅ_0r)||_1/pq}^2 1/D^2œÑ).

	Hence, the proof of Theorem 4 is completed. 
	
	

¬ß APPENDIX REFERENCE

	Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2018). Foundations of ma- chine learning. MIT press.
	0.2cm
	Newey, W. K. and McFadden, D. (1994). Large sample estimation and hypoth- esis testing. Handbook of econometrics, 4:2111‚Äì2245.
	0.2cm
	Van der Vaart, A. W. (2000). Asymptotic statistics, volume 3. Cambridge university press.
	0.2cm
	Zhou, Y., Wong, R., and He, K. (2020). Broadcasted nonparametric tensor regression.
