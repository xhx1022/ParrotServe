
Better than square-root cancellation]Better than square-root cancellation for random multiplicative functions


Department of Mathematics, Stanford University, Stanford, CA, USA
maxxu@stanford.edu




    We investigate when the better than square-root cancellation phenomenon exists for ‚àë_n‚â§ Na(n)f(n), where a(n)‚àà‚ÑÇ and f(n) is a random multiplicative function. We focus on the case where a(n) is the indicator function of R rough numbers. We prove that loglog R ‚âç (loglog x)^1/2 is the threshold for the better than square-root cancellation phenomenon to disappear.

[
    Max Wenqiang Xu
    
===================





¬ß INTRODUCTION


The study of random multiplicative functions has attracted intensive attention. Historically, they were introduced to model arithmetic functions. A Steinhaus random multiplicative function f(n) is a completely multiplicative function defined on positive integers such that f(p) are independently and uniformly distributed on the complex unit circle for all primes p. One may view it as a random model for arithmetic functions like Dirichlet characters œá(n) or n^it. Another popular model is the Rademacher random multiplicative function f(n) which was first used by Wintner<cit.> as a random model for M√∂bius function Œº(n). In this note, we focus on the Steinhaus case. The obvious dependence between random variables f(m) and f(n) whenever (m, n)‚â† 1 makes the study of random multiplicative functions intriguing.


Arguably the most striking result so far in the study of random multiplicative functions is Harper's <cit.> remarkable resolution of Helson's conjecture<cit.>, that is, the partial sums of random multiplicative functions enjoy better than square-root cancellation

    [|‚àë_n‚â§ x f(n) |] ‚âç‚àö(x)/(loglog x)^1/4,

where f(n) are random multiplicative functions. 
In particular, with the natural normalization ‚àö(x), the partial sums ‚àë_n‚â§ x f(n) do not converge in distribution to the standard complex normal distribution (see also <cit.>). Before Harper's result <cit.>, there was progress on proving good lower bounds close to ‚àö(x), e.g. <cit.>, and it was not clear that such better than square-root cancellation in (<ref>) would  appear until Harper's proof. See also recent companion work on analogous results in the character sums and zeta sums cases established by Harper <cit.>.
It is known that the better than square-root cancellation phenomenon in random multiplicative functions is connected to the ‚Äúcritical multiplicative chaos" in the probability literature. We point out references <cit.> for related discussions.  

A closely related important question in number theory is to understand the distribution of the Riemann zeta function over typical intervals of length 1 on the critical line 
‚Ñúùî¢(s)=1/2. One may crudely see the connection by viewing Œ∂(s) as a sum of n^-1/2-it for a certain range of n and n^it behaves like a Steinhaus random multiplicative function for randomly chosen t. A conjecture of Fyodorov, Hiary, and Keating (see e.g. <cit.>) suggests that there is a subtle difference between the true order of local maximal of log|Œ∂(1/2+it)| and one's first guess based on Selberg's central limit theorem for log|Œ∂(1/2+it)|. The existence of this subtle difference and the appearance of the better than square-root cancellation for random multiplicative functions both show that the corresponding nontrivial dependence can not be ignored. We refer readers to <cit.> for related discussions about partial sums of random multiplicative functions and zeta values distribution.

In this paper, we are interested in further exploring Harper's result (<ref>) and methods used there, by considering the problem in a more general context. 
 Let a(n) be a sequence in . 
When does the better than square-root cancellation phenomenon hold for ‚àë_n‚â§ N a(n)f(n), i.e.

    [|‚àë_n‚â§ Na(n) f(n)|] = o(‚àö(‚àë_n‚â§ N|a(n)|^2))Ãä?



We first make some simple observations in the situations where a(n) is ‚Äútypical" or a(n) has a rich multiplicative structure. Then we focus on a particular case where the coefficient a(n) is an indicator function of a multiplicative set. 


 ¬ß.¬ß Typical coefficients

If partial sums ‚àë_n‚â§ Na(n)f(n) with the square-root size normalization behave like the complex standard Gaussian variable, then there is just square-root cancellation. One may attempt to prove such a central limit theorem by computing the high moments, however, the moments usually blow up and such a strategy does not work here (see e.g. <cit.> for moments computation results). It turns out that for ‚Äútypical" choices of a(n), such a central limit theorem does hold. It has been carried out in the concrete case where a(n)=e^2œÄ i n Œ∏ for some fixed real Œ∏ without too good Diophantine approximation property (such Œ∏ has relative density 1 in , e.g. one can take Œ∏=œÄ) by Soundararajan and the author <cit.>, and also an average version of the result is proved by Benatar, Nishry and Rodgers <cit.>. The proof of the result in <cit.> is based on McLeish's martingale central limit theorem<cit.>, and the method was pioneered by Harper in <cit.>.  
The proof reveals the connection between the existence of such a central limit theorem and a quantity called multiplicative energy of a:= {a(n): 1‚â§ n‚â§ N} 

    E_√ó(ùêö): = ‚àë_m_1, n_1, m_2, n_2 ‚â§ N
     m_1m_2=n_1n_2a(m_1)a(m_2) a(n_1)a(n_2).

A special case of a(n) is an indicator function of a set , and the quantity E_√ó() is a popular object studied in additive combinatorics. It is now known <cit.> that a crucial condition for such a central limit theorem holds for ‚àë_n‚â§ Na(n)f(n) is that the set  has multiplicative energy ‚â§(2+œµ)||^2. See <ref> for more discussions on a(n) being a ‚Äútypical" choice.  We refer readers who are interested in seeing more examples of when a central limit theorem holds for partial (restricted) sums of random multiplicative functions to <cit.>.  



 ¬ß.¬ß Large multiplicative energy and sparse sets
 Let us focus on the case that a_n is an indicator function of a set . As we mentioned that if the set  has small multiplicative energy (among other conditions), then partial sums exhibit square-root cancellation. Suppose we purposely choose a set  with very large multiplicative energy, will it lead to better than square-root cancellation? One extreme example is = {p^n: 1‚â§ n ‚â§log_p N} being a geometric progression, where p is a fixed prime. A standard calculation gives that

    [| ‚àë_n‚àà f(n) |] =  ‚à´_0^1 |‚àë_n‚â§log_p Ne(Œ∏ n)| dŒ∏‚âçloglog N,

while [| ‚àë_n‚àà f(n) |^2] 
 = ||‚âçlog N. 
It shows that there is a great amount of cancellation in this particular example. One may also take  to be some generalized (multidimensional) geometric progression and get strong cancellation of this type. We note that the sets mentioned here with very rich multiplicative structures all have small sizes. 

Based on the initial thoughts above, we may lean toward believing that better than square-root cancellation only appears when a(n) has some particular structure that is perhaps related to multiplicativity.  
To fully answer Question¬†<ref> seems hard. The majority of the paper is devoted to a special case, where a(n) is an indicator function of a set with multiplicative features. We focus on fairly large subsets.


 ¬ß.¬ß Main results: multiplicative support

Suppose now that a(n) is a multiplicative function with |a(n)|‚â§ 1. 
The particular example we study in this paper is that a(n) is the indicator function of R-rough numbers, although the proof here may be adapted to other cases when a(n) is multiplicative.  
We write 

    _R(x): = {n‚â§ x: p|n  p‚â• R}.


By a standard sieve argument, for all 2‚â§ R‚â§ x/2 (the restriction R‚â§ x/2 is only needed for the lower bound), we have asymptotically 

    |_R(x)| ‚âçx/log R.

We expect the following threshold behavior to happen. If R is very small, the set _R(x) is close to [1,x] and better than square-root cancellation appears as in <cit.>. If R is sufficiently large, then weak dependence  may even lead to a central limit theorem. Indeed, an extreme case is that R> ‚àö(x), in which _R(x) is a set of primes and {f(n): n‚àà} is a set of independent random variables. It is natural to ask to what extent the appearance of small primes is needed to guarantee better than square-root cancellation.
Our Theorem¬†<ref> and Theorem¬†<ref> answer the question. We show that loglog R ‚âà (loglog x)^1/2 is the threshold.



  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of 
  R rough numbers up to x. For any loglog R‚â™ (loglog x)^1/2, we have   
  
    [|‚àë_n‚àà_R(x) f(n) |] ‚â™‚àö(|_R(x)|)¬∑ ( loglog R +  logloglog x/‚àö(loglog x))^1/2 .

 In particular, if loglog R = o((loglog x)^1/2), then  
   
    [|‚àë_n‚àà_R(x) f(n) |] =o ( ‚àö(|_R(x)|)).


The term logloglog x is likely removable. But for the convenience of the proof, we state the above version. See Remark¬†<ref> for more discussions.


  Let f(n) be a Steinhaus random multiplicative function and x be large. Let _R(x) be the set of 
  R rough numbers up to x. For any loglog R‚â´  (loglog x)^1/2, we have   
  
    [|‚àë_n‚àà_R(x) f(n) |] ‚â´‚àö(|_R(x)|) .


One probably can prove a lower bound of the shape ‚àö(||)¬∑(loglog R / ‚àö(loglog x))^-1/2
 when loglog R =o(‚àö(loglog x)). We do not pursue this as we focus on finding the threshold value of R instead of caring about the quantification of the exact cancellation. 


We note that one way to derive a lower bound on L^1 norm is by proving an upper bound on L^4 norm. A simple application of H√∂lder's inequality gives that

    |_R(x)| = [|‚àë_n‚àà_R(x) f(n) |^2] ‚â§([|‚àë_n‚àà_R(x) f(n) |^4])^1/3([|‚àë_n‚àà_R(x) f(n) |])^2/3.

The fourth moment  ‚â™||^2 would imply that L^1 norm ‚â´‚àö(||). However, to achieve such a bound on the fourth moment, one needs log R ‚â´ (log x)^c for some constant c, and thus this approach would not give the optimal range as in Theorem¬†<ref>.

Another reason for studying  the fourth moment (multiplicative energy) is to understand the distribution. As mentioned before, this is the key quantity that needs to be understood in order to determine if random sums have Gaussian limiting distribution, via the criteria in <cit.>. One may establish a central limit theorem in the range R‚â´exp((log x)^c) for some small positive constant c[One trick to get a smaller c than by directly computing the fourth moment over the full sum is to take the anatomy of integers into account. We refer interested readers to <cit.> to see how this idea is connected to the correct exponent in extremal sum product conjecture of Elekes and Ruzsa <cit.>.]. Interested readers are suggested to adapt the proof of <cit.>. We do not pursue results along this direction in this note. 

Theorem¬†<ref> and Theorem¬†<ref> are both proved by adapting Harper's robust method in <cit.>, with some modifications, simplifications and new observations, and we sketch the strategy with a focus on how we find the threshold. We also refer readers to a model problem in the function field case by Soundararajan and Zaman <cit.>.  The first step is to reduce the L^1 norm estimate to a certain average of the square of random Euler products. Basically, we prove that 

    [|‚àë_n‚àà f(n)|] ‚âà(x/log x)^1/2¬∑[(‚à´_-1/2^1/2 |F^(R)(1/2 + it)|^2 dt  )^1/2 ],

where F^(R)(1/2+it) := ‚àè_R‚â§ p‚â§ x (1-f(p)/p^1/2+it)^-1 is the random Euler product over primes R‚â§ p ‚â§ x. The challenging part is to give a sharp bound on the above expectation involving |F^(R)(1/2+it)|^2 for |t|‚â§ 1/2. 

We first discuss the upper bound proof. If we directly apply H√∂lder's inequality (i.e. moving the expectation inside the integral in (<ref>)), then 
we would only get the trivial upper bound  ‚â™‚àö(||) as [|F^(R)(1/2+it)|^2]‚âàlog x/log R. Harper's method starts with putting some ‚Äúbarrier events" on the growth rate of all random partial Euler products for all t. Roughly speaking, it requires that for all k,

    ‚àè_x^e^-(k+1)‚â§ p ‚â§ x^e^-k |1-f(p)/p^1/2+it|^-1¬†‚Äúgrows as expected" for all |t|‚â§ 1.

Denote such good events by ùí¢ and write s=1/2+it. By splitting the probability space based on the event ùí¢ holding or not, and applying Cauchy‚ÄìSchwarz inequality, we have 

    [(‚à´_-1/2^1/2 |F^(R)(s)|^2 dt  )^1/2 ]
       ‚âà[(‚à´_-1/2^1/21_ùí¢ |F^(R)(s)|^2 dt  )^1/2] +  [(‚à´_-1/2^1/21_ùí¢¬†fail |F^(R)(s)|^2 dt  )^1/2]  
       ‚â™[(‚à´_-1/2^1/21_ùí¢ |F^(R)(s)|^2 dt  )^1/2] + (1_ùí¢¬†fail)^1/2 ([|F^(R)(s)|^2])^1/2 .

According to the two terms above, there are two tasks that remain to be done. 

    
  * Task 1: Show that the expectation is small, conditioning on 1_ùí¢.

  * Task 2: Show that (1_ùí¢¬†fail) is sufficiently small. 
 
To accomplish task 1, Harper's method connects such an estimate to the ‚Äúballot problem" or say Gaussian random walks (see <ref>), which is used to estimate the probability of partial sums of independent Gaussian variables having a certain barrier in growth. Task 2 of estimating the probability of such good events ùí¢ happening can be done by using some concentration inequality, e.g. Chebyshev's inequality. 
Our main innovation lies in setting up ‚Äúbarrier events" in (<ref>) properly which is not the same as in <cit.>. On one hand, it should give a strong enough restriction on the growth rate of the products so that [(‚à´_-1/2^1/21_ùí¢ |F^(R)(s)|^2 dt  )^1/2] has a saving, compared to it without conditioning on 1_ùí¢. On the other hand, one needs to show that such an event ùí¢ is indeed very likely to happen which requires that the designed ‚Äúbarrier" can not be too restrictive. To make the two goals simultaneously achieved, we need loglog R = o( ‚àö(loglog x)) and this is the limit that we can push to (see Remark¬†<ref>).

The lower bound proof in Theorem¬†<ref> uses the same strategy as in <cit.> but is technically simpler. After the deduction step of reducing the problem to studying a certain average of the square of random Euler products (see (<ref>)), we only need to give a lower bound of the shape ‚â´ (log x / log R)^1/2 for the expectation on the right-hand side of (<ref>). Since the integrand |F^(R)(s)|^2 is positive, it suffices to prove such a lower bound when t is restricted to a random subset ‚Ñí. We choose ‚Ñí to be the set of t such that certain properly chosen ‚Äúbarrier events" hold.  The main difficulty is to give a strong upper bound on the restricted product [1_t_1, t_2‚àà‚Ñí|F^(R)(1/2+it_1)|^2|F^(R)(1/2+it_2)|^2] in the sense that the bound is as effective as in the ideal situation where the factors |F^(R)(1/2+it_1)|^2 and |F^(R)(1/2+it_2)|^2 are independent (see Proposition¬†<ref>), and this is also the main reason that the condition loglog R ‚â´‚àö(loglog x) is needed subject to our chosen ‚Äúbarrier events". Our proof of Theorem¬†<ref> does not involve the ‚Äútwo-dimensional Girsanov calculation", which hopefully makes it easier for readers to follow.
 




 ¬ß.¬ß Organization
 We set up the proof outline of Theorem¬†<ref> in Section¬†<ref> and defer the proof of two propositions to Section¬†<ref> and Section¬†<ref> respectively. We put all probabilistic preparations in Section¬†<ref> which will be used in the proof for both theorems. The proof of Theorem¬†<ref> is done in Section¬†<ref> and again we defer proofs of two key propositions to Section¬†<ref> and Section¬†<ref> respectively. Finally, we give more details about the ‚Äútypical" choices of a(n) in Section¬†<ref>, as well as mentioning some natural follow-up open problems. 



 ¬ß.¬ß Acknowledgement

We thank Adam Harper  for helpful discussions, corrections, and comments on earlier versions of the paper and for his encouragement. We also thank Kannan Soundararajan for the interesting discussions. The author is supported by the Cuthbert C. Hurd Graduate Fellowship in the Mathematical Sciences, Stanford. 



¬ß PROOF OF THEOREM¬†<REF>

We follow the proof strategy of Harper in <cit.>.
We establish Theorem¬†<ref> in a stronger form that for 1/2‚â§ q ‚â§ 9/10 and R in the given range loglog R ‚â™ (loglog x)^1/2,  

    [|‚àë_n‚àà_R(x) f(n) |^2q] ‚â™ |_R(x)|^q ( loglog R +  logloglog x/‚àö(loglog x))^q.

One should be able to push the range of q to 1 but for simplicity in notation, we omit it. Our interest is really about the case q=1/2.
Note that in the given range of R, by (<ref>), it is the same as proving 

    [|‚àë_n‚àà_R(x) f(n) |^2q] ‚â™(x/log R)^q ( loglog R +  logloglog x/‚àö(loglog x))^q.


The first step (Proposition¬†<ref>) is to connect the L^1 norm of the random sums to a certain average of the square of random Euler products. We define for all s with ‚Ñúùî¢(s)>0 and integers 0‚â§ k‚â§loglog x -loglog R, the random Euler products

    F_ k^(R)(s) : = ‚àè_R‚â§ p‚â§ x^e^-(k+1) (1- f(p)/p^s)^-1 = ‚àë_n‚â• 1 
     p|n R‚â§  p‚â§ x^e^-(k+1)f(n)/n^s.

We also write 

    F^(R)(s): = ‚àè_R‚â§ p‚â§ x (1- f(p)/p^s)^-1 = ‚àë_n‚â• 1 
     p|n R‚â§  p‚â§ xf(n)/n^s.

We use the notation X_2q: = ([|X|^2q])^1/2q for random variable X. 

Let f(n) be a Steinhaus random multiplicative function and x be large. Let F_k^(R)(s) be defined as in (<ref>) and loglog R ‚â™ (loglog x)^1/2. Set ùí¶: =‚åälogloglog x ‚åã. Then uniformly for all 1/2 ‚â§ q‚â§ 9/10, we have 

    ‚àë_n‚àà f(n)_2q‚â§‚àö(x/log x)‚àë_0‚â§ k ‚â§ùí¶‚à´_-1/2^1/2 | F_ k^(R)(1/2 - k/log x+it)|^2dt_q^1/2 + ‚àö(x/log x).


We remind the readers that the upper bound we aim for in Theorem¬†<ref> is very close to ‚àö(x/log R). The second term in (<ref>) is harmless since log R is much smaller than log x. 



The second step deals with the average of the square of random Euler products in (<ref>), which lies at the heart of the proof.  



Let F_k^(R)(s) be defined as in (<ref>) and loglog R ‚â™ (loglog x)^1/2. Then for all 0‚â§ k ‚â§ùí¶=‚åälogloglog x ‚åã, and uniformly for all 1/2‚â§ q‚â§ 9/10, we have 

    [(‚à´_-1/2^1/2 |F_ k^(R)(1/2 - k/log x + it)|^2dt)Ãä^q]Ãä‚â™ e^-k/2¬∑(log x/log R)Ãä^q ( loglog R + logloglog x/‚àö(loglog x))^q .






Apply Proposition¬†<ref> and Proposition¬†<ref> with q=1/2. Notice that when  loglog R ‚â™ (loglog x)^1/2, the term ‚àö(x/log x) in (<ref>) is negligible and we complete the proof. 














¬ß PROBABILISTIC PREPARATIONS

In this section, we state some probabilistic results that we need to use later. The proof can be found in <cit.> (with at most very mild straightforward modification). 


 ¬ß.¬ß Mean square calculation

We first state results on mean square calculations.


Let f be a Steinhaus random multiplicative function. Then for any 400<x‚â§ y and œÉ>-1/log y, we have

    [‚àè_x<p‚â§ y |1-f(p)/p^1/2+œÉ|^-2] = exp( ‚àë_x<p‚â§ y1/p^1+2œÉ + O(1/‚àö(x)log x) ).



The proof is basically using the Taylor expansion and the orthogonality deduced from the definition of a Steinhaus random multiplicative function. See <cit.>. 


We also quote the following result on two-dimensional mean square calculations. This will be used in proving the lower bound in Theorem¬†<ref>. 

   Let f be a Steinhaus random multiplicative function. Then for any 400<x‚â§ y and œÉ>-1/log y, we have
 
    [‚àè_x<p‚â§ y |1-f(p)/p^1/2+œÉ|^-2 |1-f(p)/p^1/2+œÉ+it|^-2 ] = exp( ‚àë_x<p‚â§ y2+2cos(tlog p)/p^1+2œÉ + O(1/‚àö(x)log x) )Ãä.
  
Moreover, if x>e^1/|t|, then we further have 

    = exp( ‚àë_x<p‚â§ y2/p^1+2œÉ + O(1) ) .



The proof of (<ref>) is in <cit.>. To deduce (<ref>), we only need to show the contribution involves cos(tlog p) terms are ‚â™ 1, which follows from a strong form of prime number theorem. See how it is done in <cit.> and <cit.>. 




 ¬ß.¬ß Gaussian random walks and the ballot problem

A key probabilistic result used in Harper's method is the following (modification of) a classical result about Gaussian random walks, which is connected to the ‚Äúballot problem". 


    Let a ‚â• 1. For any integer n > 1, let G_1, ‚Ä¶ , G_n be independent real
Gaussian random variables, each having mean zero and variance between 1/
20 and 20, say. Let
h be a function such that |h(j)| ‚â§ 10 log j. Then

    (‚àë_m=1^j G_m ‚â§ a + h(j),   ‚àÄ 1‚â§ j‚â§ n) ‚âçmin{1, a/‚àö(n)}.


Without the term h(j), it is a classical result and actually that is all we need in this paper. However, we state this stronger form as the h(j) term can be crucial if one wants to remove the logloglog x factor in Theorem¬†<ref>. We expect the random sum is fluctuating on the order of ‚àö(j) (up to step j) and so the above result is expected to be true. The quantity h(j) is much smaller compared to ‚àö(j) so it is negligible in computing the probability. 

We do not directly use the above lemma. We shall use an analogous version for random Euler products (Proposition¬†<ref>). We do the 
Girsanov-type calculation in our study. As in <cit.>, we introduce the probability measure (here x is large and |œÉ|‚â§ 1/100, say)

    (A) : = [1_A ‚àè_p‚â§ x^1/e |1-f(p)/p^1/2+œÉ|^-2  ]/[‚àè_p‚â§ x^1/e |1-f(p)/p^1/2+œÉ|^-2] .

For each ‚Ñì‚àà‚Ñï‚à™{0}, we denote the ‚Ñì-th increment of the Euler product 

    I_‚Ñì(s):= ‚àè_x^e^-(‚Ñì+2)<p‚â§ x^e^-(‚Ñì+1) (1-f(p)/p^s)^-1.

Since we are restricted to R-rough numbers n,  the parameter ‚Ñì lies in the range 0‚â§‚Ñì‚â§loglog x - loglog R. All the rest setup is exactly the same as in <cit.>. 


There is a large natural number B such that the following is true.
Let n‚â§loglog x - loglog R - (B+1), and define the decreasing sequence (‚Ñì_j)_j=1^n of non-negative integers by ‚Ñì_j = ‚åäloglog x -loglog R ‚åã -(B+1) - j. Suppose that |œÉ|‚â§1/e^B+n+1, and
that (t_j)_j=1^n is a sequence of real numbers satisfying |t_j|‚â§1/j^2/3 e^B+j+1 for all j.

Then uniformly for any large a and any function h(n) satisfying |h(n)| ‚â§ 10 log n, and with I_‚Ñì(s) defined as in (<ref>), we have  

    (-a -Bj ‚â§‚àë_m=1^jlog |I_‚Ñì_m (1/2+œÉ + it_m)| ‚â§ a + j + h(j),   ‚àÄ j‚â§ n ) ‚âçmin{1, a/‚àö(n)} .


One may view the above sum approximately as a sum of j independent random variables and each with mean ‚âà‚àë_x^e^-(‚Ñì+2)<p‚â§ x^e^-(‚Ñì+1)1/p‚âà 1 and with constant variance between 1/20 and 20. This shows the connection to Lemma¬†<ref>. The deduction of Proposition¬†<ref> from Lemma¬†<ref> can be found in the proof of <cit.>. The only modification is changing the upper bound restriction from n ‚â§loglog x -(B+1) to n ‚â§loglog x - loglog R -(B+1) and all conditions remaining satisfied. 




¬ß PROOF OF PROPOSITION¬†<REF>

The proof follows closely to the proof of <cit.>. 
For any integer 0‚â§ k ‚â§ùí¶= ‚åälogloglog x ‚åã, let 

    I_k: =(x_k+1, x_k] :=  (x^e^-(k+1) , x^e^-k].

Let P(n) be the largest prime factor of n. For simplicity, we use _n to denote the sum where the variable n is R-rough.
By using Minkowski's inequality (as 2q‚â• 1), 

    ‚àë_n‚àà f(n)_2q‚â§‚àë_0‚â§ k ‚â§ùí¶_n‚â§ x 
     P(n)‚àà I_k f(n)_2q + _n‚â§ x
    P(n)‚â§ x^e^-(ùí¶+1) f(n)_2q
    .

We first bound the last term by only using the smoothness condition and it is bounded by 
‚â§Œ® (x, x^1/loglog x  )^1/2‚â™‚àö(x) (log x)^-clogloglog  x, which is acceptable. 
The main contribution to the upper bound in (<ref>) can be written as

    = ‚àë_0‚â§ k ‚â§ùí¶‚àë_m‚â§ x 
     p|m  p ‚àà I_kf(m) _n‚â§ x/m 
     n¬†is x_k+1-smooth f(n) _2q.

We now condition on f(p) for p small but at least R. Write ^(k) to denote the expectation conditional on (f(p))_p‚â§ x_k+1. Then the above is

    = ‚àë_0‚â§ k ‚â§ùí¶ (^(k) [|‚àë_m‚â§ x
     p|m  p ‚àà I_k f(m) _n‚â§ x/m
     n¬†is x_k+1-smooth f(n)|^2q])^1/2q
       ‚â§‚àë_0‚â§ k ‚â§ùí¶ ([(^(k) [|‚àë_m‚â§ x
     p|m  p ‚àà I_kf(m) _n‚â§ x/m
     n¬†is x_k+1-smooth f(n)|^2])^q])^1/2q
        = ‚àë_0‚â§ k ‚â§ùí¶ ( [( ‚àë_m‚â§ x 
     p|m  p‚àà I_k  |_n‚â§ x/m
     n¬†is x_k+1-smooth f(n)|^2)^q] )^1/2q.

Then we only need to show that for each expectation in the sum, it is bounded as in (<ref>). Replace the discrete mean value with a smooth version. Set X=e^‚àö(log x), and we have the expectation involving primes in I_k is 

    ‚â™[(‚àë_m‚â§ x 
     p|m  p‚àà I_kX/m‚à´_m^m(1+1/X) |_n‚â§ x/t
     n¬†is x_k+1-smooth f(n)|^2 dt )Ãä^q]Ãä
        + [( ‚àë_m‚â§ x 
     p|m  p‚àà I_kX/m‚à´_m^m(1+1/X) |_x/t ‚â§ n‚â§ x/m
    n¬†is x_k+1-smooth f(n)|^2 dt )Ãä^q]Ãä.

By using H√∂lder's inequality, we upper bound the second term in (<ref>) by the q-th power of

    ‚àë_m‚â§ x 
     p|m  p‚àà I_kX/m‚à´_m^m(1+1/X) [|_x/t ‚â§ n‚â§ x/m
    n¬†is x_k+1-smooth f(n)|^2] dt .

Do the mean square calculation (<ref>) and throw away the restriction on the R-rough numbers. Then (<ref>) is at most ‚â™  2^-e^k x/log x and thus the second term in (<ref>) is ‚â™(2^-e^k x/log x)^q. Summing over k‚â§ùí¶, this is acceptable and thus we only need to focus on the first term in (<ref>). By swapping the summation, it is at most

    [ ( ‚à´_x_k+1^x |_n‚â§ x/t 
     n¬†is x_k+1-smoothf(n) |^2‚àë_t/(1+1/X)‚â§ m ‚â§ t
     p|m  p‚àà I_k  X/m dt)Ãä^q]Ãä.

We upper bound the sum over m by dropping the prime divisibility condition and using a simple sieve argument to derive that the above is at most 

    [  ( ‚à´_x_k^x |_n‚â§ x/t 
     n¬†is x_k+1-smoothf(n) |^2dt/log t)Ãä^q]Ãä = x^q[  ( ‚à´_1^x/x_k+1 |_n‚â§ z 
     n¬†is x_k+1-smoothf(n)|^2dz/z^2log(x/z))Ãä^q]Ãä,

where in the equality above we used the substitution z: =x/t. 
A simple calculation shows that we can replace log(x/z) by log x without much loss. Indeed, if z‚â§‚àö(x) then log(x/z)‚â´log x; if ‚àö(x)‚â§ z ‚â§ x/x_k+1 then log (x/z) ‚â• z^-2k/log xlog x.  Thus, we further have the bound

    ‚â™(x/log x)Ãä^q[ ( ‚à´_1^x/x_k+1 |_n‚â§ z 
     n¬†is x_k+1-smoothf(n)|^2dz/z^2-2k/log x)Ãä^q]Ãä .

To this end, we apply the following version of Parseval's identity, and its proof can be found in <cit.>.


    Let (a_n)_n=1^‚àû be any sequence of complex numbers, and let A(s): = ‚àë_n=1^‚àûa_n/n^s denote the corresponding Dirichlet series, and œÉ_c denote its abscissa of convergence. Then for any œÉ> max{0, œÉ_c}, we have 
    
    ‚à´_0^‚àû|‚àë_n‚â§ xa_n|^2/x^1+2œÉdx = 1/2œÄ‚à´_-‚àû^+‚àû|A(œÉ + it)/œÉ + it|^2 dt.


Apply Lemma¬†<ref> and the expectation in (<ref>) is

    =
    [  (‚à´_-‚àû^+‚àû|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt)Ãä^q]Ãä‚â§‚àë_n‚àà[( ‚à´_n-1/2^n+1/2|(1/2-k/log x +it)|^2/|1/2-k/log x +it|^2 dt )Ãä^q]Ãä.

Since f(m)m^it has the same law as f(m) for all m, for any fixed n we have

    [(‚à´_n-1/2^n+1/2 |(1/2-k/log x +it)|^2 dt )Ãä^q]Ãä = [ (‚à´_-1/2^1/2 |(1/2-k/log x +it)|^2 dt )Ãä^q]Ãä.

For n-1/2‚â§ t‚â§ n+1/2, we have 
1/|1/2-k/log x +it|^2‚âç 1/n^2 which is summable over n. 
We complete the proof by inserting the above estimates into (<ref>). 








¬ß PROOF OF PROPOSITION¬†<REF>


This is the key part of the proof that reveals how loglog R ‚âà‚àö(loglog x) could become the transition range. 
We begin with a discretization process which is the same as in <cit.>.  For each |t|‚â§1/2, set t(-1)=t, and then iteratively for each 0‚â§ j ‚â§log(log x /log R) -2 define 

    t(j): = max{u‚â§ t(j-1): u = n/((log x) /e^j+1)log ((log x) /e^j+1)¬†for some n‚àà}.

By the definition, we have <cit.>

    |t-t(j)|‚â§2/((log x /e^j+1)log ((log x)/e^j+1).

Given this notation, let B be the large fixed natural number from Proposition¬†<ref>. Let ùí¢(k) denote the event that for all |t|‚â§1/2 and for all k‚â§ j ‚â§loglog x - loglog R -B -2, we have

    (log x/e^j+1log R e^C(x)  )^-1‚â§‚àè_‚Ñì = j ^‚åäloglog x -loglog R ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +it(‚Ñì)) | ‚â§log x/e^j+1log R e^C(x),

where notably, our C(x) is chosen as the 





    C(x):=loglog R + 100 logloglog x.

We shall establish the following two key propositions. The first proposition says that when we are restricted to the good event ùí¢(k), the q-th moment is small. 

Let x be large and loglog R ‚â™ (loglog x)^1/2. Let 
C(x) be defined as in (<ref>). Let F_k^(R) be defined as in (<ref>) and ùí¢(k) be defined as in (<ref>). For all 0‚â§ k ‚â§ùí¶ = ‚åälogloglog x‚åã and 1/2‚â§ q ‚â§ 9/10, we have 

    [(‚à´_-1/2^1/21_ùí¢(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)Ãä^q]Ãä‚â™( log x /e^klog R)Ãä^q ( C(x)/‚àö(loglog x))^q.


The second proposition is to show that indeed 1_ùí¢(k) happens with high probability. 


Let ùí¢(k) be defined as in (<ref>). For all 0‚â§ k ‚â§ùí¶= ‚åälogloglog x‚åã and uniformly for all 1/2‚â§ q ‚â§ 9/10 and
C(x) defined in (<ref>), we have

    (ùí¢(k)¬†fails) ‚â™ e^-C(x).
 

The above two key propositions imply Proposition¬†<ref>. 

   According to the good event ùí¢(k) happening or not, we have 
   
    [(‚à´_-1/2^1/2  |F_ k^(R)(1/2 - k/log x + it)|^2dt)Ãä^q]Ãä
       ‚â§[(‚à´_-1/2^1/21_ùí¢(k) |F_ k^(R)(1/2 - k/log x + it)|^2dt)Ãä^q]Ãä +   [(‚à´_-1/2^1/21_ùí¢(k)fails |F_ k^(R)(1/2 - k/log x + it)|^2dt)Ãä^q]Ãä
       ‚â§( log x /e^klog R)Ãä^q ( C(x)/‚àö(loglog x))^q +   (‚à´_-1/2^1/2 [|F_ k^(R)(1/2 - k/log x + it)|^2]dt )Ãä^q(ùí¢(k)¬†fails)^1-q,

  where in the first term we used Proposition¬†<ref> and we applied H√∂lder's inequality with exponents 1/q, 1/1-q to get the second term. We next apply the mean square calculation (<ref>) to derive that the above is 
    
    ‚â™( log x /e^klog R)Ãä^q(  ( C(x)/‚àö(loglog x))^q +  (ùí¢(k)¬†fails)^1-q)Ãä.

  Plug in the definition of C(x) and use Proposition¬†<ref> with 1-q ‚â• 1/10 (and then the exceptional probability to the power 1/10 is negligible) to deduce that 
  
    ‚â™   e^-k/2( log x /log R)Ãä^q¬∑ ( C(x)/‚àö(loglog x))^q,

  which completes the proof. 



We remark that in (<ref>), the quantity C(x)= loglog R +100 logloglog x is different from just being a constant C in <cit.>. The reason for our choice of C(x) is the following. Firstly, to keep the q-th moment in Proposition¬†<ref> has a saving (i.e. to make ( C(x)/‚àö(loglog x))^q small), we require that C(x)= o(‚àö(loglog x)).  Secondly, it turns out that in order to make the exceptional probability in Proposition¬†<ref> small enough, one has the constraint loglog R ‚â™ C(x). The combination of the above two aspects together leads to loglog R =o(‚àö(loglog x)). 




   In the deduction of Proposition¬†<ref>, we did not use an iterative process as used in <cit.>. Instead, we added an extra term 100logloglog x for the purpose of getting strong enough bounds on (ùí¢(k)¬†fails). We simplified the proof by getting a slightly weaker upper bound in Theorem¬†<ref> as compensation.





 ¬ß.¬ß Proof of Proposition¬†<ref>



The proof of Proposition¬†<ref> is a simple modification of the proof of Key Proposition 1 in <cit.>. We emphasize again the main difference is instead of using a large constant C as in <cit.> but replacing it with C(x) defined in (<ref>), and we do not need the extra help from the quantity h(j) which hopefully makes the proof conceptually easier.  

By using H√∂lder's inequality, it suffices to prove that 

    [‚à´_-1/2^1/2 |F_k^(R)(1/2-k/log x + it)|^2dt ]‚â™ e^-k¬∑log x/log R¬∑C(x)/‚àö(loglog x) ,

uniformly for 0‚â§ k ‚â§ùí¶ = ‚åälogloglog x ‚åã and 1/2 ‚â§ q‚â§ 9/10. 
We can upper bound the left-hand side of (<ref>) by 

    ‚â§‚à´_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt

where  is the event that 

    (log x/e^j+1log R e^C(x)  )^-1‚â§‚àè_‚Ñì = j ^‚åäloglog x -loglog R ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +it(‚Ñì)) | ‚â§log x/e^j+1log R e^C(x)

for all k‚â§ j ‚â§loglog x -loglog R -B -2. This is an upper bound as  is the event of  holds for all |t|‚â§1/2. By the fact that f(n) has the same law as f(n)n^it, we have

    ‚à´_-1/2^1/2[ |F_k^(R)(1/2-k/log x + it)|^2] dt = ‚à´_-1/2^1/2[ |F_k^(R)(1/2-k/log x )|^2 ]dt,

where  denotes the event that 

    (log x/e^j+1log R e^C(x)  )^-1‚â§‚àè_‚Ñì = j ^‚åäloglog x -loglog R ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +i(t(‚Ñì)-t)) | ‚â§log x/e^j+1log R e^C(x),

for all k‚â§ j ‚â§loglog x- loglog R - B -2. 
 We next apply Proposition¬†<ref>. 






It is clear that ‚Ñã(k,t) is the event treated in Proposition¬†<ref> with n=‚åäloglog x- loglog R ‚åã -(B+1)-k; œÉ= k/log x and t_m = t(‚åäloglog x- loglog R ‚åã -(B+1) -m)-t for all m; and 

    a = C(x) + B+1,    h(j)=0.

The parameters indeed satisfy |œÉ|‚â§1/e^B+n+1 and |t_m|‚â§1/m^2/3e^B+m+1
for all m. Apply Proposition¬†<ref> to derive

    [ |F_k^(R)(1/2-k/log x)|^2]/[|F_k^(R)(1/2-k/log x)|^2] = (‚Ñã(k,t)) ‚â™min{ 1, a/‚àö(n)}.

A simple mean square calculation (see (<ref>)) gives that 

    [|F_k^(R)(1/2-k/log x)|^2] = exp(‚àë_R‚â§ p ‚â§ x^e^-(k+1)1/p^1-2k/log x +O(1))Ãä‚â™log x/e^klog R.

Combining the above two inequalities and the relation in (<ref>), we get the desired upper bound for the quantity in (<ref>). Thus, we complete the proof of (<ref>) and Proposition¬†<ref>.  






 ¬ß.¬ß Proof of Proposition¬†<ref>

In the proof, we will see why it is necessary to make C(x) large enough compared to loglog R. The proof starts with the union bound. We have 

    (ùí¢(k)¬†fails) ‚â§_1 +_2,

where

    _1 = ‚àë_k‚â§ j ‚â§log (log x/log R) -B-2( ‚àè_‚Ñì = j ^‚åälog (log x/log R) ‚åã-B-2
    |I_‚Ñì(1/2-k/log x + i t(‚Ñì)) | >log x/e^j+1log R e^C(x)¬†for some t)Ãä

and 

    _2 = ‚àë_k‚â§ j ‚â§log (log x/log R) -B-2( ‚àè_‚Ñì = j ^‚åälog (log x/log R) ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +i t(‚Ñì)) |^-1 >log x/e^j+1log R e^C(x)¬†for some t)Ãä,

where |t|‚â§ 1/2. 
We focus on bounding _1, and _2 can be estimated similarly. Replace the set of all |t|‚â§ 1/2 by the discrete set 

    ùíØ(x, j): = {n/((log x)/e^j+1) log ((log x)/e^j+1) : |n|‚â§ ((log x)/e^j+1) log ((log x)/e^j+1)  }Ãä,

and apply the union bound to get 

    _1 = ‚àë_k‚â§ j ‚â§log (log x/log R) -B-2
     t(j) ‚ààùíØ(x,j)( ‚àè_‚Ñì = j ^‚åälog (log x/log R) ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +it(‚Ñì)) | >log x/e^j+1log R e^C(x))Ãä.

By using Chebyshev's inequality this is at most 

    ‚â§‚àë_k‚â§ j ‚â§log (log x/log R) -B-2
     t(j) ‚ààùíØ(x,j)1/(log x/e^j+1log R e^C(x))^2[ ‚àè_‚Ñì = j ^‚åälog (log x/log R) ‚åã-B-2
    |I_‚Ñì(1/2-k/log x +it(‚Ñì)) |^2  ].

Since f(n) and f(n)n^it have the same law, the above is 

    ‚â™‚àë_k‚â§ j ‚â§log (log x/log R) -B-2| ùíØ(x,j)|/(log x/e^j+1log R e^C(x))^2[ ‚àè_‚Ñì = j ^‚åälog (log x/log R) ‚åã-B-2
    |I_‚Ñì(1/2-k/log x ) |^2  ].

The expectation here is, again through a mean square calculation (<ref>), ‚â™log x/e^j+1log R. Note |ùíØ(x, j)| ‚â§ ((log x)/e^j+1) log ((log x)/e^j+1). 
We conclude that 

    _1 ‚â™‚àë_k‚â§ j ‚â§log (log x/log R) -B-2e^loglog R-2C(x) + loglog ( log x / e^j+1)   ‚â™ e^-C(x),

where in the last step we used that C(x)=   loglog R + 100 logloglog x. Thus we complete the proof of Proposition¬†<ref>. 






¬ß PROOF OF THEOREM¬†<REF>

 We first notice that if R>x^1/A for any fixed large constant A, then _R(x) is a set of elements with only O_A(1) number of prime factors. This would immediately imply that [|‚àë_n‚àà f(n)|^4] ‚â™_A ||^2 and by (<ref>), the conclusion follows. From now on, we may assume that 

    R‚â§ x^1/A.

The proof strategy of Theorem¬†<ref> again follows from <cit.>. The main differences lie in the design of the barrier events and taking advantage of R being large.  In particular, we do not need a ‚Äútwo-dimensional Girsanov-type" calculation which makes our proof less technical. 
We first do the reduction step to reduce the problem to understanding certain averages of random Euler products, as in the upper bound proof. 

There exists a large constant C such that the following is true. Let x be large and loglog R ‚â´‚àö(loglog x). Let F^(R)(s) be defined as in (<ref>). Then,
uniformly for all 1/2 ‚â§ q‚â§ 9/10 and any large V, we have ‚àë_n‚àà f(n)  _2q

    ‚â´‚àö(x/log x)( ‚à´_-1/2^1/2 | F^(R)(1/2 +4V/log x + it)|^2 dt_q^1/2  - C/e^V‚à´_-1/2^1/2 | F^(R)(1/2 +2V/log x + it)|^2 dt_q^1/2 -C ).



The remaining  tasks are to give a desired lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2 and  an upper bound on F^(R)(1/2 +2V/log x + it)_q^1/2. 
The upper bound part is simple. Indeed, simply apply H√∂lder's inequality and do a mean square calculation (<ref>) to get

    [(‚à´_-1/2^1/2|F^(R)(1/2 +2V/log x + it)|^2  dt)^q] ‚â™(‚à´_-1/2^1/2[|F^(R)(1/2 +2V/log x + it)|^2]  dt )^q‚â™(log x/Vlog R)^q .


We next focus on the main task, giving a good lower bound on F^(R)(1/2 +4V/log x + it)_q^1/2. For each t‚àà‚Ñù, 
we use L(t) denote the event for all ‚åälog V ‚åã +3 ‚â§ j ‚â§loglog x - loglog R -B -2, the following holds

    (log x/e^j+1log R e^D(x)  )^-B‚â§‚àè_‚Ñì = j ^‚åäloglog x -loglog R ‚åã-B-2
    |I_‚Ñì(1/2+4V/log x +it) | ‚â§log x/e^j+1log R e^D(x),

where D(x):= c‚àö(loglog x -loglog R) with 

    c= 1/4min{loglog R/‚àö(loglog x-loglog R) , 1 }‚âç 1.

We are now ready to define a random set 

    ‚Ñí: = {-1/2‚â§ t ‚â§ 1/2: L(t)¬†defined by (<ref>) holds}.

It is clear that

    [(‚à´_-1/2^1/2|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] ‚â•[(‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt)^q].

We use the following estimate and defer its proof to Section¬†<ref>.

Let x be large and loglog R ‚â´‚àö(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let ‚Ñí be the random set defined in (<ref>). Then uniformly for any 1/2‚â§ q‚â§ 9/10, we have

    [(‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] ‚â´(log x/Vlog R)^q .


Plug (<ref>), (<ref>) and (<ref>) into Proposition¬†<ref> with q=1/2
(and choosing V to be a sufficiently large fixed constant so that C/e^V kills the implicit constant) to get that 
 
    [|‚àë_n‚àà_R(x) f(n) |] ‚â´‚àö(|_R(x)|) .

This completes the proof of Theorem¬†<ref>.




¬ß PROOF OF PROPOSITION¬†<REF>


The proof proceeds the same as in <cit.> (see also <cit.>) and we provide a self-contained proof here and highlight some small modifications. 

Let P(n) denote the largest prime factor of n as before. We have assumed that (<ref>) holds, e.g. R‚â§‚àö(x) (This restriction is not crucial but makes the notation later easier). Let œµ denote a Rademacher random variable independent of f(n), and recall that  indicates that the variable n under the summation is R rough. For 1/2‚â§ q‚â§ 9/10, we have 

    [|_n‚â§ x
     P(n)>‚àö(x)f(n) |^2q]     = 1/2^2q[|_n‚â§ x
     P(n)‚â§‚àö(x)f(n) + _n‚â§ x
     P(n)>‚àö(x)f(n)+_n‚â§ x
     P(n)>‚àö(x)f(n)-_n‚â§ x
     P(n)‚â§‚àö(x)f(n)|^2q]
       ‚â§[|_n‚â§ x
     P(n)‚â§‚àö(x)f(n) + _n‚â§ x
     P(n)>‚àö(x)f(n)|^2q] + [|_n‚â§ x
     P(n)>‚àö(x)f(n)-_n‚â§ x
     P(n)‚â§‚àö(x)f(n)|^2q]
        = 2[|œµ_n‚â§ x
     P(n)>‚àö(x)f(n) + _n‚â§ x
     P(n)‚â§‚àö(x)f(n)|^2q] = 2[|_n‚â§ x f(n)|^2q],

where the last step we used the law of 
    œµ_n‚â§ x
     P(n)>‚àö(x)f(n)= œµ‚àë_‚àö(x)<p‚â§ x f(p) _m‚â§ x/p f(m)
 conditional on (f(p))_R‚â§ p ‚â§‚àö(x) is the same as the law of _n‚â§  x
 P(n)>‚àö(x)f(n). By the above deduction, it suffices to give a lower bound on _n‚â§ x
 P(n)>‚àö(x)f(n)_2q. 
Do the decomposition

    _n‚â§ x
     P(n)>‚àö(x)f(n) =‚àë_‚àö(x)‚â§ p ‚â§ x f(p) _m‚â§ x/pf(m).

The inner sum is determined by (f(p))_R‚â§ p‚â§‚àö(x) and apply the Khintchine's inequality <cit.> to get

    [|_n‚â§ x
     P(n)>‚àö(x)f(n)|^2q] ‚â´[(‚àë_‚àö(x)< p ‚â§ x |_m‚â§ x/pf(m)|^2 )^q] ‚â•1/(log x)^q[(‚àë_‚àö(x)< p ‚â§ xlog p¬∑|_m‚â§ x/pf(m)|^2)^q].

Next, do the smoothing step as we did in the upper bound case. Again set X = e^‚àö(log x). 
Write 

    ‚àë_‚àö(x)< p‚â§ xlog p ¬∑ |_m‚â§ x/pf(m)|^2 = ‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X) |_m‚â§ x/p f(m)|^2dt.

One has |a+b|^2‚â• a^2/4 - min{|b|^2, |a/2|^2}‚â• 0 and thus the above is at least 

    1/4‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X) |_m‚â§ x/t f(m)|^2dt 
        -‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X)min{|_x/t‚â§ m‚â§ x/p f(m)|^2, 1/4 |_m‚â§ x/t f(m)|^2}.

It follows that the quantity we are interested in has the lower bound

    [|_n‚â§ x
     P(n)>‚àö(x)f(n)|^2q] ‚â•   1/(log x)^q[(1/4‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X) |_m‚â§ x/t f(m)|^2dt)^q] 
        - 1/(log x)^q[(‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X) |_ x/t<m‚â§ x/p f(m)|^2dt)^q].

Use H√∂lder's inequality and throw away the R-rough condition to upper bound the subtracted term in (<ref>) by

    ‚â§1/(log x)^q(‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X)[ |‚àë_ x/t<m‚â§ x/p f(m)|^2]dt)Ãä^q
       ‚â™1/(log x)^q(‚àë_‚àö(x)<p‚â§ x log p ¬∑  (x/pX +1) )^q‚â™1/(log x)^q(xlog x/X +x)^q‚â™ (x/log x)^q.

The first term in (<ref>) (without the factor 1/4(log x)^q) is 

    [(‚àë_‚àö(x)<p‚â§ xlog p ¬∑X/p‚à´_p^p(1+1/X) |_m‚â§ x/t f(m)|^2dt)^q] 
        = [(‚à´_‚àö(x)^x_t/1+1/X <p‚â§ t log p ¬∑X/p|_m‚â§ x/tf(m)|^2 dt )^q]
       ‚â´ [(‚à´_‚àö(x)^x|_m‚â§ x/tf(m)|^2dt )^q] = x^q[(‚à´_1^‚àö(x)  |_m‚â§ z f(m) |^2dz/z^2)^q].

To this end, we impose the smooth condition to invert the sums to Euler products. We have for any large V, 

    [(‚à´_1^‚àö(x)  |_m‚â§ z f(m) |^2dz/z^2)^q] ‚â•[(‚à´_1^‚àö(x)  |_m‚â§ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]
       ‚â•[(‚à´_1^+‚àû  |_m‚â§ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- [(‚à´_‚àö(x)^+‚àû  |_m‚â§ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]
       ‚â•[(‚à´_1^+‚àû  |_m‚â§ z
     x-smooth f(m) |^2dz/z^2+8V/log x)^q]- 1/e^2Vq[(‚à´_1^+‚àû  |_m‚â§ z
     x-smooth f(m) |^2dz/z^2+4V/log x)^q].

Apply Lemma¬†<ref> to get that the first term is 

    ‚â´[(‚à´_-1/2^1/2|F^(R)(1/2 + 4V/log x + it)|^2 dt )^q].

For the second term, an application of Lemma¬†<ref> gives

    ‚â™ e^-2Vq[(‚à´_-‚àû^+‚àû|F^(R)(1/2 + 2V/log x + it)/|1/2 + 2V/log x + it|^2 dt )^q] ‚â™ e^-2Vq[(‚à´_-1/2^1/2 |F^(R)(1/2 + 2V/log x + it)|^2 )^q]

where in the last step we used the fact that f(n)n^it has the same law as f(n) and ‚àë_n ‚â• 1 n^-2 converges. Bounds in (<ref>) and (<ref>) together give the desired bound for the first term in (<ref>) and we complete the proof. 





¬ß PROOF OF PROPOSITION¬†<REF>

In this section, 
we prove Proposition¬†<ref>. The proof significantly relies on the following proposition, which is a mean value estimate of the product of |F^(R)(œÉ + it_1)|^2
and |F^(R)(œÉ + it_2)|^2. Our upper bound matches the guess if you pretend the two products are independent. 

Let x be large and loglog R ‚â´‚àö(loglog x). Let F^(R)(s) be defined as in (<ref>) and V be a large constant. Let ‚Ñí be the random set defined in (<ref>). Then we have
    
    [(‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt)^2]  ‚â™ (log x/Vlog R)^2.






The proof starts with an application of H√∂lder's inequality. We have 

    [(‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt)^q] ‚â•([‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt])^2-q/([(‚à´_‚Ñí|F^(R)(1/2 +4V/log x + it)|^2  dt)^2])^1-q.

Proposition¬†<ref> gives a desired upper bound for the denominator. We next give a lower bound on the numerator. By using that f(n)n^it has the same law as f(n), the numerator is

    (‚à´_-1/2^1/2 [1_L(t)|F^(R)(1/2 +4V/log x + it)|^2]  dt)^2-q= ([ 1_L(0)|F^(R)(1/2 +4V/log x )|^2] )^2-q .

We next use Proposition¬†<ref> by taking n=‚åäloglog x - loglog R ‚åã - (B+1) - ‚åälog V ‚åã, a =D(x)=c‚àö(loglog x -loglog R ) and h(j)=0 to conclude that (1_L(0))‚â´ 1. Combining with the mean square calculation¬†(<ref>), we have

    [1_L(0)|F^(R)(1/2 +4V/log x + it)|^2] ‚â´(1_L(0))¬∑ [|F^(R)(1/2 +4V/log x + it)|^2] ‚â´log x/V log R.

We complete the proof by plugging (<ref>) and (<ref>) into (<ref>). 


The proof of Proposition¬†<ref> is a bit involved and its proof is inspired by <cit.> and <cit.>. We are not using the ‚Äútwo-dimensional Girsanov-type" computation as used in <cit.> which significantly simplified the proof. We do not expect any further savings when R is as large as stated in Proposition¬†<ref> while for a smaller R, one might expect there could be further cancellation as in <cit.> which may be verified by adapting the ‚Äútwo-dimensional Girsanov-type" calculation. 



    Expand the square and it equals
    
    [ ‚à´_-1/2^1/21_L(t_1) |F^(R)(1/2 +4V/log x + it_1)|^2 dt_1 ‚à´_-1/2
        ^1/21_L(t_2)|F^(R)(1/2 +4V/log x + it_2)|^2 dt_2   ].

   By using that f(n)n^it has the same law as f(n), we write the above as (t:= t_1-t_2)
   
    ‚à´_-1^1 [1_L(0) |F^(R)(1/2 +4V/log x )|^21_L(t)|F^(R)(1/2 +4V/log x + it)|^2 ] dt.

For |t| large enough, the two factors behave independently, which is the easier case. Indeed,  if |t|>1/log R, drop the indicator functions and bound the corresponding integration by

    ‚â™max_ 1/log R < |t|‚â§ 1 [  |F^(R)(1/2 +4V/log x)|^2¬∑ |F^(R)(1/2 +4V/log x + it)|^2   ]  .
 
 Apply the two dimensional mean square calculation (<ref>) with (x, y)=(R, x)  to conclude that the above is 
 
    ‚â™(log x/V log R)^2.

 
We next focus on the case |t|‚â§ 1/log R. Since f(p) are independent of each other, we can decompose the Euler products into pieces and analyze their contributions to (<ref>) separately.
Define the following three sets of primes based on the sizes of primes

    ùí´_1: = {p¬†prime: R‚â§ p < x^e^-(‚åäloglog x -loglog R ‚åã-B-2)},


    ùí´_2: = {p¬†prime:  x^e^-(‚åäloglog x -loglog R ‚åã-B-2)‚â§ p ‚â§ x^e^-(‚åälog V ‚åã +3)},

and 

    ùí´_3: = {p¬†prime: x^e^-(‚åälog V ‚åã +3) < p ‚â§ x }.

We proceed as follows. Note that the events L(0) and L(t) are irrelevant to f(p) for p‚ààùí´_1 ‚à™ùí´_3. For partial products over primes p‚ààùí´_1 ‚à™ùí´_3, we directly do mean square calculations.
For partial products over primes p‚ààùí´_2, we will crucially use the indicator functions 1_L(0) and 1_L(t) defined in (<ref>) with j= ‚åälog V ‚åã +3. This separation gives that the integration in (<ref>) over |t|‚â§ 1/log R is

    ‚à´_|t|‚â§1/log R[‚àè_p‚ààùí´_1 ‚à™ùí´_3  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2] 
    √ó[1_L(0)1_L(t)‚àè_p‚ààùí´_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt.

We first upper bound the expectation over primes in ùí´_1 ‚à™ùí´_3 uniformly over all t. By using independence between f(p) and (<ref>), we can bound it as 

    ‚â™exp( ‚àë_p‚ààùí´_14/p^1+8V/log x + ‚àë_p‚ààùí´_34/p^1+8V/log x).

By simply using the prime number theorem and the definition of ùí´_1 and ùí´_3, one has that both sums in (<ref>)  are ‚â™ 1 so that (<ref>) is ‚â™ 1, where we remind readers that B is a fixed constant. Now our task is reduced to establishing the following

    ‚à´_|t|‚â§1/log R[1_L(0)1_L(t)‚àè_p‚ààùí´_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt ‚â™(log x/V log R)^2.


Our strategy would be, roughly speaking, using the barrier event 1_L(t) to bound certain partial products involved with t directly and then use the mean square calculation to deal with the rest of the products. The exact partial products that we will apply barrier events would depend on the size of t.  

We first do a simple case, which helps us get rid of the very small t, say |t|<V/log x. We use the the condition 1_L(t) and pull out the factors related to L(t) to get that the contribution from |t|<V/log x is at most 

    ‚â™‚à´_|t|‚â§V/log x e^2c‚àö(loglog x- loglog R)¬∑ (log x/V log R)^2¬∑[1_L(0)‚àè_p‚ààùí´_2  |1-f(p)/p^1/2 +4V/log x|^-2]  dt 
       ‚â™V/log x¬∑  e^2c‚àö(loglog x- loglog R)¬∑(log x/V log R)^2¬∑[‚àè_p‚ààùí´_2  |1-f(p)/p^1/2 +4V/log x|^-2] 
       ‚â™(log x/V log R)^2,

where in the second last step we dropped the 1_L(0) condition, and in the last step we applied (<ref>) together with log R ‚â•exp(4c  ‚àö(loglog x)) where c is defined in (<ref>). Thus we only need to establish the following 

    ‚à´_V/log x‚â§ |t|‚â§1/log R[1_L(0)1_L(t)‚àè_p‚ààùí´_2  |1-f(p)/p^1/2 +4V/log x|^-2 |1-f(p)/p^1/2 +4V/log x+it|^-2]  dt ‚â™(log x/V log R)^2 .


We now enter the crucial part where we will apply the barrier events according to the size of |t|.
We decompose the set ùí´_2 into two parts according to |t|.
For each fixed V/log x‚â§ |t| ‚â§ 1/log R, we write 

    ùí´_2 = ùíÆ(t) ‚à™‚Ñ≥(t),

where 

    ùíÆ(t):={p¬†prime:  x^e^-(‚åäloglog x -loglog R ‚åã-B-2)‚â§ p ‚â§ e^V/|t|},

and 

    ‚Ñ≥(t):= {p¬†prime:  e^V/|t|‚â§ p ‚â§ x^e^-(‚åälog V ‚åã +3)}.

The set of primes ùíÆ(t) would be those we will apply barrier events and ‚Ñ≥(t) would be estimated by a mean square calculation. Note that for p‚àà‚Ñ≥(t), there is a nice decorrelation as we needed in (<ref>) due to that p‚â• e^V/|t|. 
Let us now see how such a decomposition of ùí´_2 would help us. We use a local notation

    G(p, t): = |1-f(p)/p^1/2 + 4V/log x+it|^-2.

Then the quantity in (<ref>) is the same as 

    ‚à´_V/log x‚â§ |t|‚â§1/log R[1_L(0)1_L(t)‚àè_p‚ààùí´_2 G(p, 0) ‚àè_p‚ààùíÆ(t) G(p, t) ‚àè_p‚àà‚Ñ≥(t) G(p, t)  ]  dt.

We apply the barrier events condition 1_L(t) to bound the product over p‚ààùíÆ(t) so that the above is at most 

    ‚â™(V/log R)^2¬∑ e^2c‚àö(loglog x - loglog R)¬∑‚à´_V/log x‚â§ |t|‚â§1/log R1/t^2[1_L(0)‚àè_p‚ààùí´_2 G(p, 0) ‚àè_p‚àà‚Ñ≥(t) G(p, t)  ]  dt.

We next upper bound the expectation in (<ref>) uniformly for all V/log x‚â§ |t| ‚â§ 1/log R. We first drop the indicator function and rewrite the product based on the independence between f(p) to derive that

    [1_L(0)‚àè_p‚ààùí´_2 G(p, 0) ‚àè_p‚àà‚Ñ≥(t) G(p, t)]‚â§[ ‚àè_p‚ààùíÆ(t) G(p, 0)] ¬∑ [‚àè_p‚àà‚Ñ≥(t) G(p, 0)G(p, t)].

 Use the mean square calculation results in (<ref>) and (<ref>) to further get an upper bound on the expectation
 
    ‚â™V/|t|/log R¬∑(tlog x/V^2)^2‚â™|t|(log x)^2/V^3log R .

Now we plug the above bound to (<ref>) to get that (<ref>) is crudely bounded by 

    (log x/log R)^2¬∑e^2c‚àö(loglog x- loglog R)/Vlog R¬∑‚à´_V/log x‚â§ |t|‚â§1/log R1/|t| dt ‚â™(log x/log R)^2.

In the last step we used that log R ‚â•exp(4c  ‚àö(loglog x)) where c is defined in (<ref>).  This completes the proof of (<ref>) and thus the proof of the proposition. 




¬ß CONCLUDING REMARKS




 ¬ß.¬ß Typical behavior and small perturbations

We give a sketch of the situation when a(n) itself is independently and randomly chosen. 
We write 

    a(n) = r(n) X(n)

where r(n)>0 is deterministic and X(n) are independently distributed with [|X(n)|^2]=1. We may naturally assume that there is some r such that

    r(n) ‚âç r(m) ‚âç r

for all n, m, i.e. no particular random variable would dominate the whole sum in size. One may also just assume r=1 throughout the discussion here. 
We claim that for typical X(n), the random sums satisfy the sufficient condition established in <cit.> on having a Gaussian limiting distribution.

The key condition one needs to verify is that almost surely (in terms of over X(n)), we have 

    R_N(a) : =‚àë_ m_i, n_j‚â§ N 
    
     m_i‚â† n_j 
     m_1m_2=n_1n_2   a(n_1)a(n_2) a(m_1) a(m_2) = o(r^4N^2).

The proof of (<ref>) is straightforward. By using the divisor bound, we know there are ‚â™ N^2+œµ number of quadruples (m_1, m_2, n_1, n_2) under the summation. If we expect some square-root cancellation among a(n_1)a(n_2) a(m_1) a(m_2), then R_N(a) above should be around r^4N^1+ typically. 
Indeed, by using the fact that all a(n) are independent, we have the L^2 bound 

    [|R_N|^2] = [R_N R_N] ‚â™ r^8 N^2+.

This leads to, 
almost surely (in terms of over X(n)), that we have 

    R_N(a) = o(r^4N^2).

To this end, by using <cit.>, almost surely, we have a central limit theorem for the random partial sums of a Steinhaus random multiplicative function.
See <cit.> for a closely related result where they used the method of moments.  




In Question¬†<ref>, we asked if it is possible to characterize the choices of a(n) that give better than square-root cancellation. On one hand, as discussed above, we know for typical a(n), there is just square-root cancellation. On the other hand, if a(n) is a deterministic multiplicative function taking values on the unit circle, then by the fact that a(n)f(n) has the same distribution as f(n) and the result established by Harper (<ref>), the partial sums ‚àë_n‚â§ N a(n)f(n) have better than square-root cancellation. Our main theorems study one particular example of multiplicative nature.  Combining these observations, 
we believe that any small perturbation coming from a(n) that destroys the multiplicative structure would make the better than square-root cancellation in (<ref>) disappear. We ask the following question in a vague way as a sub-question of Question¬†<ref>.

Is it true that the only ‚Äúessential choice" of a(n) leading to better than square-root cancellation is of multiplicative nature? 




 ¬ß.¬ß Threshold in other settings and the limiting distribution

The main theorems of this paper prove that there is square-root cancellation for loglog R ‚â´ (loglog x)^1/2. What is the limiting distribution then? We have remarked earlier that one may establish a central limit theorem when R‚â´exp((log x)^c) for some  constant c<1 by understanding the corresponding multiplicative energy. It becomes less clear for smaller R. 


 What is the limiting distribution of  ‚àë_n‚àà_R(x) f(n) with ‚Äúproper" normalization, for all ranges of R?   


We finally comment that there is another family of partial sums that naturally has the threshold behavior for better than square-root cancellation. Let = [x, y] with y‚â§ x. We would like to know for what range of y, typically, 

    ‚àë_x‚â§ n ‚â§ x+y f(n) = o(‚àö(y)).

We believe one can adapt the argument here to find that the threshold behavior is around log (x/y) ‚âà‚àö(loglog x). It is certainly interesting to understand the limiting distribution for the short interval case thoroughly, beyond the previous result in <cit.>.  






plain

